Adversarial evaluation stress tests a model's understanding of natural
language and exposes what makes an underlying task difficult and
interesting.
Existing adversarial analysis exposes superficial patterns and
annotation artifacts for question answering.
In contrast, our human-in-the-loop adversarial generation process
shows human authors how the system is answering questions in an
interactive interface.
We apply the framework to a question answering task called Quizbowl,
with an enthusiastic, skilled cadre of question authors.
We validate the resulting questions via live human--computer
tournaments: although the questions appear ordinary to humans, they
systematically stump both neural and information retrieval models.
The adversarial questions cover diverse phenomena, spanning multi-hop
reasoning to entity type distractors, exposing open challenges in
robust question answering.
