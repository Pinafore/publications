\section{Conclusion}
\label{sec:future}

One of the challenges of machine learning is knowing
why systems fail.  This work brings together two threads that
attempt to answer this question: visualizations and adversarial
examples.  Visualizations underscore the capabilities of existing models,
while adversarial examples---crafted with the ingenuity of human
experts---show that these models are still
far from matching human prowess. 

Our experiments with both neural and \abr{ir} methodologies show that \abr{qa} models
still struggle with synthesizing clues, handling distracting information,
and adapting to unfamiliar data.  Our \challenge{} dataset is only the first of
many iterations~\cite{ruef16build}. As models improve, future \challenge{}
datasets can elucidate the limitations of next-generation \abr{qa} systems.

While we focus on \abr{qa}, our procedure is applicable to
other \abr{nlp} settings where there is (1) a pool of talented authors
who (2) write text with specific goals. Future research
can look to craft \challenge{} datasets for other \abr{nlp} tasks
that meet these criteria.