
\section{Evaluation Results}
\label{sec:eva}
This sections discusses our evaluation results.
% Four experiments are conducted to evaluate \name{} on its graph coverage, QA accuracy, the contributions of its components, and types of its failures.


\subsection{Graph Coverage}
The statistics of the constructed free-text entity graph in the three datasets are shown in Table~\ref{tab:coverage}. 

The free-text entity graph has high coverage.
On average each question is connected to more than 1500 candidate nodes in the graph, and 90\% of them can be answered by the graph.
After filtering, still more than 80\% questions can be answered.
In comparison, we manually examined 50 randomly sampled QBLink questions, only 38\% of them can be answered within two hops in the DBpedia graph.

The free-text entity graph is dense. On average there are 5 and 12 edges connecting a correct answer nodes to the question nodes in \qblink{} and \qanta{}. \trivia{} questions on average have 2 entities and more than one is connected to the correct answer.
Each edge has 8-15 evidence sentences---a lot information for the model to reason upon.

The free-text entity graph naturally separates the correct answer by its structure. Compared to incorrect answers (-1), the correct ones (+) are connected by significantly more edges. and their edges are attached with more evidence sentences.

Aided by free-text evidence, the coverage of the structured graph is no longer the bottleneck. The free-text entity graph provides much evidence and sets up the graph-based QA well for success.





\subsection{Answer Accuracy}
Table~\ref{tab:results} lists the answer accuracy of \name{} and baselines, on ALL questions or groups with different number of entities.
% In addition to the overall results (ALL), it also shows the accuracy on questions with different number of entities (1-3, 4-6, 7-9, 10+).



\name{} outperforms all baselines, on all datasets and all sub groups.
It significantly outperforms DrQA and DocQA, the reading comprehension baselines. Organizing evidence to semantic structures and performing QA on the structures are beneficial.
It outperforms its own BERT-Sent filtering, showing the strength of graph-based reasoning over text-based answer ranking.
Compared to MemNN, which uses the same evidence but in a bag-of-evidence fashion, \name{}'s structured reasoning thrives on complex questions in \qblink{} and \qanta{}. On \trivia{}, which has less than two edges per node and not many structures to use; \name{} performs similar to MemNN, as expected.


Comparing model accuracy among questions with different number of entities, there is a significant trend on \name{} that its accuracy increases dramatically on questions with more entities. In comparison, almost all other methods' effectiveness stay flat, even when there is more evidence brought in by additional question entities. It shows \name{}'s strong ability to leverage richer evidence signals with its structured reasoning capability, while  previous methods might be confused by additional evidence, as they do not explicitly capture the semantic structure.




% performance on \name{}.  


\input{2019_acl_graph_qa/data/ablation.tex}



\subsection{Ablation Study}
Our ablation studies are conducted on \qblink{} and \name{}-GloVe.
For each of the ablation model, one component is removed and we keep the other
settings.
The ablation results are listed in Table~\ref{tab:ablation}.
% We conduct ablation studies on various components of \name{} on QBLink and \name{}-GloVe.

The pruning of the free-text graph is very useful. The edge evidence filtering has to be kept, other the GNN is too slow to train.
The node filtering is very crucial. Many of the nodes do not need complex reasoning and can be filtered out by single evidence; mixing them together not only slow down the GNN learning but also misguide it to focus on those simpler cases.

Both node evidence (gloss) and edge evidence sentences are important for \name{}'s effectiveness. Each contributes to relatively 10\% to its accuracy. 
The edge importance scoring, which controls the weights of the information flow from question nodes to answer nodes, is not as crucial. The node and edge representations are already learned and weighted based on the question. The current model does not yet model the self-attention between question nodes, thus edges connecting different question nodes are not that different. 


% Next we evaluate the importance of each model's component. We use \qblink{}
% as the benchmark dataset. For entity filter, we evaluate on the full
% entity graph.
% And to evaluate the component of \name{}, we use focused graph 
% on the development set (i.e, not count the questions which 
% correct answer is filtered for generating the graph).
% We report the results in Table~\ref{tab:ablation}.

% We start by making predictions on full list of entities, without filtering
% the graph, the result is significantly worse. Most errors come from wrong entity
% types, since the model does not
% include any constraints over type information.




% All the features contribute to the final \name{} model. Without question,
% the performance significantly degrades to $44.4$. Wikipedia description
% is also essential, otherwise the information is too weak to get accurate 
% entity representation. The result degrades to $65.3$ without textual evidences, 
% since the model lacks essential information to make correct prediction.
% The first example in Table~\ref{tab:ablation} shows that the Wikipedia description of answer
% %  ``Works and Days'' is irrelevant to the question,  thus we need the edge evidence to get it. 
% The edge importance score also helps. In the second example of Table~\ref{tab:ablation},
% if all edges are treated equally, the model would get confused between different `` Chief Justice'',
% such as ``William Rehnquist''. To get the correct answer, the model should learn that the edge from 
% ``Roe v. Wade'' to ``Warren E. Burger'' is more important to find the correct answer.


\input{2019_acl_graph_qa/sections/72-error.tex}