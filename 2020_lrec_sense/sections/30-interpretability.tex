\section{Evaluating Interpretability}
\label{sec:intp}

We turn to traditional evaluations of sense embeddings later
(Section~\ref{sec:wordsim}), but our focus is on human
interpretability.
If you
show a human the senses, can they understand why a model would assign
a sense to that context?
This section evaluates whether the
representations make sense to human consumers of multisense models.

In the age of \abr{bert} and \elmo{}, these are the dimensions that are most critical for
multisense representations.
While contextual word vectors are most useful for \emph{computer}
understanding of meaning, \emph{humans} often want an overview of word
meanings for other tasks.

Sense representations are useful for human-in-the-loop
applications.
They help understand semantic drift~\cite{hamilton-16}: how
do the meanings of ``gay'' reflect social progress?  They help people
learn languages~\cite{noraset-17}: what does it mean when someone says
that I ``embarrassed'' them?
They
help linguists understand the sense inventory of a
language~\cite{kawahara-14}: what are the frames that can be used by
the verb ``participate''?
These questions (and human understanding)
are helped by {\bf discrete senses}, which the Gumbel softmax uncovers.

More broadly, this is the goal of interpretable machine
learning~\cite{doshi-velez-17}.
While downstream models do not always
need an interpretable explanation of \emph{why} a model uses a particular
representation, interactive machine learning and explainable machine
learning do.
To date, multisense representations ignore this use case.

\begin{table}[t!]
	\centering
	\small
	\begin{tabular}{cccc}
		\toprule
		\multirow{2}{*} {Model} &  Sense  & Judgment & \multirow{2}{*} {Agreement}  \\
		&  Accuracy  & Accuracy&  \\
		\midrule
		\midrule
		\abr{muse}  & 67.33 & 62.89 & 0.73 \\
		\abr{mssg-30k}  & 69.33 & 66.67 & 0.76 \\
		\gasi{-$\beta$} & \textbf{71.33} &\textbf{ 67.33}& \textbf{0.77}\\
		\bottomrule
	\end{tabular} 
	\caption{Word intrusion evaluations on top ten nearest neighbors of sense embeddings.  Users find misfit words most easily with \gasi{-$\beta$}, suggesting these representations are more interpretable.}
	\label{tab:intrusion}
	
\end{table}

\paragraph{Qualitative analysis}

Previous papers use nearest neighbors of a few examples to qualitatively
argue that their models have captured meaningful senses of words.
We also give an example in Figure~\ref{fig:tsne}, which provides an
intuitive view on how the learned senses are clustered by visualizing
the nearest neighbors of word ``bond'' using t-\abr{sne}
projection~\citep{maaten2008visualizing}.
Our model (right) disentangles the three sense of ``bond'' clearly.

However, examples can be cherry-picked. This
problem bedeviled topic modeling until 
rigorous human evaluation was introduced~\citep{chang-09b}.
We adapt both
aspects of their evaluations: \emph{word
  intrusion}~\citep{schnabel-15} to evaluate whether individual senses
are coherent and \emph{topic intrusion}---rather sense intrusion in
this setting---to evaluate whether humans agree with models' sense
assignments \emph{in context}.
Using crowdsourced evaluations from
Figure-Eight, we compare our models with two previous
state-of-the-art sense embeddings models, i.e.,
\abr{mssg}~\citep{neelakantan2015efficient} and
\abr{muse}~\citep{Muse}.\footnote{\abr{mssg} has two settings; we run human
  evaluation with \abr{mssg}-30K which has higher correlation
  with MaxSimC on \abr{scws}.}

\begin{table}[t]
	\small
	\begin{tabular}{cccc}
		\toprule
		Model &  Accuracy & $P$ & Agreement \\
		\midrule
		\midrule
		\abr{muse} & 28.0 & 0.33 &0.68\\
		\abr{mssg-30K} &  44.5 & 0.37 &0.73 \\
		\gasi{ (no $\beta$)} &  33.8 & 0.33& 0.68\\
		\gasi{-$\beta$} &  \textbf{50.0} & \textbf{0.48}& \textbf{0.75}\\
		\midrule
		\gasi{-$\beta$}-pruned & \textbf{75.2} &\textbf{0.67}& \textbf{0.96}\\
		\bottomrule
	\end{tabular} 
	\centering 
	\caption{Human-model consistency on \emph{contextual word
            sense selection}; $P$ is the average probability assigned
          by the model to the human choices. \gasi{-$\beta$} is most
          consistent with crowdworkers. Reducing sense duplication by
          retraining our model with pruning mask improves human-model
          agreement.}
	\label{tab:human}
	
\end{table}


\subsection{Word Intrusion for Sense Coherence}
\label{sec:word_intrusion}

\citet{schnabel-15} suggest a ``good'' word embedding should have
coherent neighbors and evaluate coherence by \emph{word
  intrusion}.
They present crowdworkers four words: three are close
in embedding space but one is an ``intruder''.
If the
embedding makes sense, contributors will easily spot the word that
``does not belong''.

Similarly, we examine the coherence of ten nearest neighbors of senses
in the \emph{contextual word sense selection} task
(Section~\ref{sec:sense_selection}) and replace one neighbor with an
``intruder''.
We generate three
intruders for each sense and collect three judgments per intruder.
To account for variation in users and intruders, we count an instance
as ``correct'' if two or more crowdworkers correctly spot the
intruder.




Like \newcite{chang-09b}, we want the ``intruder'' to be about
as frequent as the target but not too similar.
For sense~$s_i^{m}$ of word type~$w_i$, we randomly select
a word from the neighbors of \emph{another} sense $s_i^{n}$ of $w_i$.

All models have comparable model accuracy.  \gasi{-$\beta$} learns
senses that have the highest coherence while \abr{muse} learns
mixtures of senses (Table~\ref{tab:intrusion}).

We use the aggregated confidence score provided by Figure-Eight to
estimate the level of \textbf{inter-rater agreement} between multiple
contributors~\cite{f8-confidence}.
The agreement is high for all models, and \gasi{-$\beta$} has the
highest agreement, suggesting that the senses learned by
\gasi{-$\beta$} are easier to interpret.


\subsection{Contextual Word Sense Selection}
\label{sec:sense_selection}

The previous task measures whether individual senses are
coherent.
Now we evaluate models' disambiguation of senses \emph{in context}.

\paragraph{Task Description} 

Given a target word in context, we ask a crowdworker to select which
sense group best fits the sentence.
Each sense group is described by its top ten distinct nearest
neighbors, and the sense group order is shuffled.

 
\paragraph{Data Collection}  

We select fifty nouns with five sentences from SemCor
3.0~\citep{miller1994using}.
We first filter all word types with fewer than ten sentences and
select the fifty most polysemous nouns from WordNet~\citep{wordnet2}
among the remaining senses.
For each noun, we randomly select five sentences.

 
\paragraph{Metrics} 

For each model, we collect three judgments for each question.
We consider a model correct if at least two crowdworkers select the
same sense as the model.


\begin{table}
	\small
	\begin{tabular}{ccccc}
		\toprule
		&  &\abr{muse}& \abr{mssg}& \gasi{-$\beta$}\\
		\midrule
		
		\multirow{2}{*} {\parbox{1.3cm}{\centering word \\overlap}} &agree& 4.78 & 0.39&1.52\\
		&disagree&  5.43 & 0.98&6.36 \\
		\midrule
		\multirow{2}{*} {\parbox{1.3cm}{\centering Glove\\ cosine  }} &agree&  0.86& 0.33& 0.36\\
		&disagree &  0.88 & 0.57& 0.81\\
		\bottomrule
	\end{tabular} 
	\centering 
	
	\caption{Similarities of human and model choices when they
		agree and disagree for two metrics: simple word overlap (top) and
		Glove cosine similarity (bottom).  Humans agree with the model when
		the senses are distinct.}
	
	\label{tab:dist} 
	 
\end{table}


\begin{table}[t!]
	\small
	\centering
	\rowcolors{2}{gray!25}{white}
	\begin{tabular}{cp{6.5cm}}
		\toprule
		\multicolumn{2}{p{7.5cm}}{The real \underline{question} is - how are those four years used and what is their value as training?}   \\
		\midrule
		{\bf s1}& hypothetical, unanswered, topic, answered, discussion, yes/no, answer, facts\\
		s2& toss-up, answers, guess, why, answer, trivia, caller, wondering, answering\\
		s3& argument, contentious, unresolved, concerning, matter, regarding, debated, legality\\
		\bottomrule
	\end{tabular} 
	\centering 
	\caption{A case where \abr{mssg} has low overlap but confuses raters (agreement 0.33); the model chooses s1.  }
	\label{tab:mssg_ea} 
	
	 
\end{table}


 
\paragraph{Sense Disambiguation and Interpretability} 

If humans consistently pick the same sense as the model, they must
first understand the choices, thus implying the nearest neighbor words
were coherent.
Moreover, they also agree that among those senses, that sense was the
right choice for this token.
\gasi{-$\beta$} selections are most consistent with humans';
its selections have the highest accuracy and assigns the largest probability
assigned to the human choices (Table~\ref{tab:human}).
Thus,
\gasi{-$\beta$} produces sense embeddings that are both more
interpretable and distinguishable. \gasi{} without a scaling factor,
however, has low consistency and flat sense distribution.


 
\paragraph{Model Confidence}

However, some contexts are more ambiguous than others.
For fine-grained senses, best practice is to use graded sense
assignments~\cite{erk-13}.
Thus, we also show the model's probability of the top human choice; distributions
close to $\frac{1}{K}$ (0.33) suggest the model learns a distribution
that cannot disambiguate senses.
We consider granularity of senses further in \secref{sec:related}.


 
\paragraph{Inter-rater Agreement} 

We use the confidence score computed by Figure-Eight to estimate the
raters' agreement for this task. \gasi{-$\beta$} has
the highest human-model agreement, while both \abr{Muse} and \gasi{}
without scaling have the lowest.





 
\paragraph{Error Analysis}

Next, we explore why crowdworkers disagree with the model even though
the senses are interpretable (Table~\ref{tab:intrusion}).
Is it that the model has learned \emph{duplicate} senses that both the
users and model cannot distinguish (the senses are all bad or
identical) or is it that crowdworkers agree with each other but
\emph{disagree} with the model (the model selects bad senses)?

Two trends suggest duplicate senses cause disagreement both for humans
with models and humans with each other.
For two measures of sense similarity---simple word overlap and
\glove{} similarity---similarity is lower when users and models agree
(Table~\ref{tab:dist}).
Humans also agree with each other more.  For \gasi{}-$\beta$, pairs with
perfect agreement have a word overlap of around 2.5, while the senses
with lowest agreement have overlap around 5.5.

To reduce duplicated senses, we
retrain the model with pruning (Section~\ref{sec:sasi}, Equation~\ref{eq:dup}).
We remove a little more than one sense per type on average.
To maintain the original setting, for word types that have fewer than three
senses left, we compute the nearest neighbors to dummy senses 
represented by random embeddings.
Our model trained with pruning mask 
(\gasi-$\beta$-pruned) reaches very high inter-rater agreement and higher
human-model agreement than models with 
a fixed number of senses (Table~\ref{tab:human}, bottom).