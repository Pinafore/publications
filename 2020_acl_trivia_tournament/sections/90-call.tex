
\section{A Call to Action}
\label{sec:call}



You may disagree with the superiority of \qb{} as a \qa{} framework (even for trivia nerds, not all agree\dots \textit{de gustibus non est disputandum}).

In this final section, we hope to distill our advice into a call to action regardless of your question format of choice.

Here are our recommendations if you want to have an effective leaderboard.

\subsection{Talk to Trivia Nerds (and everybody else)}

You should talk to trivia nerds.

It's not just that they're lonely, but they also have useful information (not just about the election of 1876).

The skill of trivia is not just to collect this information but to see connections between these disparate facts~\citep{jennings-06}.

These skills are exactly those that we want computers to develop.

They're writing these questions anyway; we can save money and time if we pool resources.

Computer scientists can benefit if the trivia community writes questions that aren't trivial for computers to solve (e.g., avoiding quotes and named entities).

The trivia community can benefit from tools that make their job easier: show related questions, link to Wikipedia articles, or predict where humans will answer the question.

And, above all, give the trivia community credit.

For example, if you use trivia questions scraped from websites to build a trivia \qa{} dataset, you should contact the original authors to make sure it is okay and thank them in the acknowledgements.  

Likewise, the broader public has unique knowledge and skills.

In contrast to low-paid crowdworkers, public platforms for question answering and fun citizen science~\citep{bowser-13} have revealed how much expertise is available for free\dots if you can engage the relevant communities.

These sites also reward high-quality answers through upvotes and engagement (although whether a good answer implies factual accuracy is an open question).

For example, the \text{Quora} query ``Is there a nuclear control room on nuclear aircraft carriers?'' is purportedly answered by someone who worked in such a room~\citep{humphries-17}.

There should also be deeper, more frequent discussion of actual questions within the \abr{nlp} community.

Part of every post-mortem of trivia tournaments is a detailed discussion of the questions, where good questions are praised and bad questions are excoriated.

This is not meant to shame the askers but rather to help build and reinforce cultural norms: questions should be well-written, precise, and fulfill the creator's goals.

Just like trivia tournaments, \abr{qa} datasets resemble a product for sale.

Creators want people to invest considerable time and sometimes money (e.g., \abr{gpu} hours) in using their data and submitting to their leaderboards.

It is ``good business'' to build a reputation for quality questions and being willing to discuss individual questions.

\subsection{Make your Questions Discriminative}

As we argued in Section~\ref{sec:discriminative}, you should maximize the proportion of questions that are discriminative.

While we argue that the \qb{} format potentially allows every question to be discriminative, we recognize that not everyone is crazy enough to adopt this (beautiful) format.

For more traditional \abr{qa} tasks, however, you can still maximize the usefulness of your dataset by making sure as many questions as possible are challenging (but not impossible) for today's \abr{qa} systems.

\subsection{Eat Your Own Dog Food}

As you develop new question answering tasks, you should feel comfortable playing the task as a human.

Importantly, this is not just to replicate what crowdworkers are doing (also important) but to validate that there are not hidden assumptions, that the metrics are fair, and that the task is well defined.

For this to feel real, you will need to keep score; have all of your coauthors participate and compare their scores.

Again, we emphasize that human and computer skills are not identical, but this is a benefit: humans natural aversion to unfairness will help you create a better task, while computers will blindly optimize a broken objective function~\citep{bostrom-03}.

As you go through the process of playing on your question--answer dataset, you can see where you might have fallen short on the goals we outline in Section~\ref{sec:craft}.

Again, we do not harbor the illusion that everyone will adopt \qb{} as a format.

But you can use some of the intuitions to make your data more discriminative.

For example, in visual \abr{qa}, you can offer increasing resolutions of the image: the better system should be able to do more with less.

For other settings, you can achieve pyramidality by adding additional metadata: coreference, semantic parses, refinement, disambiguation, or for table-based \abr{qa}, the correct column mappings.

In short, consider multiple versions/views of your data that progress from extremely difficult to easy.

This not only makes more of your dataset discriminative but also helps reveal what makes a question answerable, i.e. what key word or information solves the puzzle~\cite[Chapter 10]{klagge-10}.

\subsection{Embrace Multiple Answers or Specify Specificity}

As \qa{} moves to more complicated formats and answer candidates, what constitutes a correct answer becomes more complicated.

Fully automatic evaluations are valuable for both training and quick-turnaround evaluation.

In the case of answers where annotators may disagree about an answer, it becomes more important to explicitly state what level of specificity is required (e.g., \underline{September 1, 1939} vs. \underline{1939} or \underline{Lenninism} vs. \underline{socialism}).

Or, if not all questions have a single answer, link answers to a knowledge base with multiple surface forms or explicitly enumerate which multiple answers are acceptable.

However, with more complicated systems and evaluations, a return to the yearly evaluations of \abr{trecqa} may be the best option.

This improves not only the quality of evaluation (we can have real-time human judging) but also lets the test set reflect the build it/break it cycle~\citep{ruef-16}, as attempted by the 2019 iteration of \abr{fever}.

Moreover, another lesson the \abr{qa} community could learn from trivia games is to turn it into a spectacle: exciting games with a telegenic host.

This has a benefit to the public, who see how \abr{qa} systems fail on difficult questions, as well as to \abr{qa} researchers, who have a spoonful of fun sugar to encourage them to look at their systems' output as well as the output of competitors.

In between are automatic metrics which mimic the flexibility of human raters.

This could take inspiration from evaluations for machine translation~\citep{papineni-02,specia-10} or summarization~\citep{lin-04}.

\subsection{Truth in Evaluation and Advertising}

While---particularly for leaderboards---it is tempting to turn everything into a single number, recognize that there are often different sub-tasks and types of players who deserve recognition.

A simple model that requires less training data or runs in under ten milliseconds may be objectively more useful than a bloated, brittle monster of a system that has a slightly higher \fone{}.

While you may only rank by a single metric (this is what trivia tournaments do too), you may want to recognize the highest-scoring model that was built by undergrads, took no more than one second per second, was trained only on Wikipedia, etc.

Make realistic claims about human--computer comparisons.

If your task is realistic, fun, and challenging, then you will be able to find skilled humans to play against your computer agents.

They will not only give you ``real'' human baseline numbers but can also tell you how to fix your question answering dataset\dots after all, they've been at it longer than you have.

