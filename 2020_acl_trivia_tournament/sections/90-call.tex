

\section{A Call to Action}
\label{sec:call}


You may disagree with the superiority of \qb{} as a \qa{} framework (\textit{de gustibus non est disputandum}).
In this final section, we hope to distill our advice into a call to action regardless of your question format or source.
Here are our recommendations if you want to have an effective leaderboard.

\paragraph{Talk to Trivia Nerds}

You should talk to trivia nerds because they have useful information (not just about the election of 1876).
Trivia is not just the accumulation of information but also connecting disparate facts~\cite{jennings-06}.
These skills are exactly those we want computers to develop.

Trivia nerds are writing questions anyway; we can save money and time
if we pool resources.\footnote{Many question answering datasets
  benefit from trivia community efforts.
Ethically using the data, however, requires acknowledging their
contributions and using their input to create datasets~\cite[Consent
  and Inclusivity]{jo-20}.}
Computer scientists benefit if the trivia community writes questions
that aren't trivial for computers to solve (e.g., avoiding quotes and
named entities).
The trivia community benefits from tools that make their job easier:
show related questions, link to Wikipedia, or predict where humans
will answer.

Likewise, the broader public has unique knowledge and skills.
In contrast to low-paid crowdworkers, public platforms for question
answering and citizen science~\cite{bowser-13} are brimming with free
expertise if you can engage the relevant communities.
For example, the \text{Quora} query ``Is there a nuclear control room
on nuclear aircraft carriers?'' is purportedly answered by someone who
worked in such a room~\cite{humphries-17}.
As machine learning algorithms improve, the ``good enough''
crowdsourcing that got us this far may not be enough for
continued progress.

\paragraph{Eat Your Own Dog Food}

As you develop new question answering tasks, you should feel comfortable playing the task.
Importantly, this is not just to replicate what crowdworkers are doing (also important) but to remove hidden assumptions, institute fair metrics, and define the task well.
For this to feel real, you will need to keep score; have all of your coauthors participate and compete.

Again, {\bf human and computer skills are not
  identical}, but this is a benefit: humans' natural aversion to
unfairness will help you create a better task, while computers will
blindly optimize an objective function~\cite{bostrom-03}.
As you go through the process of playing on your question--answer dataset, you can see where you might have fallen short on the goals from Section~\ref{sec:craft}.

\paragraph{Won't Somebody Look at the Data?}

After \abr{qa} datasets are released, there should also be deeper, more frequent discussion of actual questions within the \abr{nlp} community.
Part of every post-mortem of trivia tournaments is a detailed discussion of the questions, where good questions are praised and bad questions are excoriated.
This is not meant to shame the writers but rather to help build and reinforce cultural norms: questions should be well-written, precise, and fulfill the creator's goals.
Just like trivia tournaments, \abr{qa} datasets resemble a product for sale.
Creators want people to invest time and sometimes money (e.g., \abr{gpu} hours) in using their data and submitting to their leaderboards.
It is ``good business'' to build a reputation for quality questions and discussing individual questions.

Similarly, discussing and comparing the actual predictions made by the competing systems should be part of
any competition culture---without it, it is hard to tell what a couple of points
on some leaderboard mean.  To make this possible, we recommend that leaderboards include an
easy way for anyone to download a system's development predictions for qualitative analyses.

\paragraph{Make Questions Discriminative}

We argue that questions should be discriminative (Section~\ref{sec:discriminative}), and while \qb{} is one solution (Section~\ref{sec:qb}), not everyone is crazy enough to adopt this (beautiful) format.
For more traditional \abr{qa} tasks, you can maximize the usefulness of your dataset by ensuring as many questions as possible are challenging (but not impossible) for today's \abr{qa} systems.

But you can use some \qb{} intuitions to improve discrimination.
In visual \abr{qa}, you can offer increasing resolutions of the image.
For other settings, create pyramidality by adding metadata: coreference, disambiguation, or alignment to a knowledge base.
In short, consider multiple versions/views of your data that progress from difficult to easy.
This not only makes more of your dataset discriminative but also reveals what makes a question answerable.

\paragraph{Embrace Multiple Answers or Specify Specificity}

As \qa{} moves to more complicated formats and answer candidates, what constitutes a correct answer becomes more complicated.
Fully automatic evaluations are valuable for both training and quick-turnaround evaluation.
In the case annotators disagree, the question should explicitly state what level of specificity is required (e.g., \underline{September 1, 1939} vs. \underline{1939} or \underline{Leninism} vs. \underline{socialism}).
Or, if not all questions have a single answer, link answers to a knowledge base with multiple surface forms or explicitly enumerate which answers are acceptable.

\paragraph{Appreciate Ambiguity}

If your intended \abr{qa} application has to handle ambiguous questions,
do justice to the ambiguity by making it part of your task---for example, recognize the
original ambiguity and resolve it (``did you mean\dots'') instead of giving credit
for happening to `fit the data'.

To ensure that our datasets properly ``isolate the property that
motivated [the dataset] in the first place''~\cite{Zaenen-2006}, we
need to explicitly appreciate the unavoidable ambiguity instead of
silently glossing over it.\footnote{Not surprisingly, `inherent'
  ambiguity is not limited to \abr{qa}; \citet{pavlick-19} show
  natural language inference has `inherent disagreements'
  between humans and advocate for recovering the full range of
  accepted inferences.}

This is already an active area of research, with conversational \abr{qa} being a new setting
actively explored by several datasets~\cite{reddy-18,choi-18}; and other work explicitly focusing on
identifying useful clarification questions~\cite{rao-2018}, thematically linked questions~\cite{elgohary-18} or resolving ambiguities that arise from
coreference or pragmatic constraints by rewriting underspecified question strings~\cite{elgohary-19,min-20}.

\paragraph{Revel in Spectacle}

However, with more complicated systems and evaluations, a return to the yearly evaluations of \abr{trecqa} may be the best option.
This improves not only the quality of evaluation (we can have real-time human judging) but also lets the test set reflect the build it/break it cycle~\cite{ruef-16}, as attempted by the 2019 iteration of \abr{fever}~\cite{thorne-19}.
Moreover, another lesson the \abr{qa} community could learn from
trivia games is to turn it into a spectacle: exciting games with a
telegenic host.
This has a benefit to the public, who see how \abr{qa} systems fail on difficult questions and to \abr{qa} researchers, who have a spoonful of fun sugar to inspect their systems' output and their competitors'.

In between full automation and expensive humans in the loop are automatic metrics that mimic the flexibility of human raters, inspired by machine translation evaluations~\cite{papineni-02,specia-10} or summarization~\cite{lin-04}. 
However, we should not forget
that these metrics were introduced as `understudies'---good enough when quick evaluations are needed for system
building but no substitute for a proper evaluation.
In machine translation, \newcite{laubli-20} reveal that crowdworkers cannot spot the errors that neural \abr{mt} systems make---fortunately, trivia nerds are cheaper than professional translators.

\paragraph{Be Honest in Crowning \abr{qa} Champions}

Leaderboards are a ranking over entrants based on a ranking over numbers.
This can be problematic for several reasons.
The first is that single numbers have some variance; it's better to communicate estimates with error bars.

While---particularly for leaderboards---it is tempting to turn everything into a single number, there are often different sub-tasks and systems who deserve recognition.
A simple model that requires less training data or runs in under ten milliseconds may be objectively more useful than a bloated, brittle monster of a system that has a slightly higher \fone{}~\cite{dodge-19}. 
While you may only rank by a single metric (this is what trivia tournaments do too), you may want to recognize the highest-scoring model that was built by undergrads, took no more than one second per example, was trained only on Wikipedia, etc.

Finally, if you want to make human--computer comparisons, pick the right humans.  Paraphrasing
a participant of the 2019 \abr{mrqa} workshop~\cite{fisch-19}, a system better than the average human at brain surgery does not imply superhuman performance in brain surgery.  
Likewise, beating a distracted crowdworker on \abr{qa} is not \abr{qa}'s endgame.  
If your task is realistic, fun, and challenging, you will find experts to play against your computer.
Not only will this give you human baselines worth reporting---they can also tell you how to fix your \abr{qa} dataset\dots after all, they've been at it longer than you have.
