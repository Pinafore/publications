\section{Surprise, this is a Trivia Tournament!}
\label{sec:tournament}


``My research isn't a silly trivia tournament,'' you say.
That may be, but let us first tell you a little about what running a tournament is like, and perhaps you might see similarities.


First, the questions.
Either you write them yourself or you pay someone to write questions by a particular date (sometimes people on the Internet). 

Then, you advertise.  
You talk about your questions:
who is writing them, what subjects are covered, and why people should try to answer them.

Next, you have the tournament. 
You keep your questions secure until test time, collect answers from all participants, and declare a winner. 
Afterward, people use the questions to train for future tournaments.

These have natural analogs to crowd sourcing questions, writing the paper, advertising, and running a leaderboard. 
Trivia nerds cannot help you form hypotheses or write your paper, but they can tell you how to run a fun, well-calibrated, and discriminative tournament.

Such tournaments are designed to effectively find a winner, which matches the scientific goal of knowing which model best answers questions.
Our goal is {\bf not to encourage the \abr{qa} community to adopt to quirks and gimmicks of trivia games}.
Instead, it's to encourage experiments and datasets that {\bf consistently and efficiently find the systems that best answer questions}.

\subsection{Are we Having Fun?}
\label{sec:fun}

Many authors use crowdworkers to establish human accuracy~\cite{rajpurkar-16,choi-18}.
However, they are not the only humans who should answer a dataset's questions.
So should the datasets' creators.

In the trivia world, this is called a {\bf play test}:
get in the shoes of someone \emph{answering} the questions.
If you find them boring, repetitive, or uninteresting, so will
crowdworkers.
If you can find shortcuts to answer questions~\cite{rondeau-18,
  kaushik-18}, so will a computer.

Concretely, \newcite{weissenborn-17} catalog artifacts in \squad{}~\cite{rajpurkar-18}, the most popular \abr{qa} leaderboard.
If you see a list like ``Along with Canada and the United Kingdom, what country\dots'', you can ignore the rest of the question and just type Ctrl+F~\cite{yuan-19, russell-20} to find the third country---\underline{Australia} in this case---that appears with ``Canada and the \abr{uk}''.
Other times, a \squad{} playtest would reveal frustrating questions that are 
i) answerable given the information but not with a direct span,\footnote{A source paragraph says ``In [Commonwealth countries]\dots the term is generally restricted to\dots Private education in North America covers the whole gamut\dots''; thus, ``What is the term private school restricted to in the US?'' has the information needed but not as a span.}
ii) answerable only given facts beyond the given paragraph,\footnote{A source paragraph says ``Sculptors [in the collection include] Nicholas Stone, Caius Gabriel Cibber, [...], \underline{Thomas Brock}, Alfred Gilbert, [...] and Eric Gill'', i.e., a list of names; thus, the question ``Which British sculptor whose work includes the Queen Victoria memorial in front of Buckingham Palace is included in the V\&A collection?'' should be unanswerable in \squad{}.}
iii) unintentionally embedded in a discourse, resulting
in arbitrary correct answers,\footnote{A question ``Who \emph{else} did Luther use violent rhetoric towards?'' has the gold answer ``writings condemning the Jews and in diatribes against \underline{Turks}''.}
iv)  or non-questions.

\searchqa{}~\cite{dunn-17}, derived from \jeopardy{}, asks ``An article that he wrote about his riverboat days was eventually expanded into \textit{Life on the Mississippi}.''
The apprentice and newspaper writer who wrote the article is named Samuel Langhorne Clemens; however, the reference answer is his later pen name, \underline{Mark Twain}.
Most \qa{} evaluation metrics would count \underline{Samuel Clemens} as incorrect.
In a real game of \jeopardy{}, this would not be an issue (Section~\ref{sec:ambiguity}).

Of course, fun is relative, and any dataset is bound to contain errors.
However, playtesting is an easy way to find systematic problems: unfair, unfun playtests make for ineffective leaderboards.
Eating your own dog food can help diagnose artifacts, scoring issues, or other shortcomings early in the process.

The deeper issues when creating a \abr{qa} task are:
i) have you designed a task that is internally consistent,
ii) supported by a scoring metric that matches your goals,
iii) using gold annotations that reward those who do the task well?
Imagine someone who loves answering the questions your task poses: would they have fun on your task?
This is the foundation of Gamification~\cite{ahn-06}, which can create quality data from users motivated by fun rather than pay.
Even if you pay crowdworkers, unfun questions may undermine your dataset goals.

\subsection{Am I Measuring what I Care About?}
\label{subsection:measuring-what-you-care-about}



Answering questions requires multiple skills: identifying answer mentions~\cite{hermann-15}, 
naming the answer~\cite{yih-15}, abstaining when necessary~\cite{rajpurkar-18}, and justifying an answer~\cite{fever-18}.
In \qa{}, the emphasis on \abr{sota} and leaderboards has focused attention on single automatically computable
metrics---systems tend to be compared by their `\squad{} score' or their `\abr{nq} score', as if this were all there
is to say about their relative capabilities.  Like \abr{qa} leaderboards, trivia tournaments need to decide
on a single winner, but they explicitly recognize that there are more interesting comparisons.

A tournament may recognize differnt background/resources---high school, small school, undergraduates~\cite{naqt-eligibility}.  Similarly, more practical leaderboards would reflect training time
or resource requirements~\citep[see][]{dodge-19} including `constrained' or `unconstrained'
training~\citep{bojar-2014}.
Tournaments also give specific awards (e.g., highest score without incorrect
answers).  Again, there are obvious leaderboard analogs that would go beyond a single number.  
In \squad{} 2.0~\cite{rajpurkar-18}, abstaining contributes the same
to the overall \fone{} as a fully correct answer, obscuring whether a system
is more precise or an effective abstainer.  If the task
recognizes both abilities as important, reporting a single score risks implicitly prioritizing one balance of the two.

\subsection{Do my Questions Separate the Best?}
\label{sec:discriminative}

Assume that you have picked a metric (or a set of metrics) that captures what you care about.
A leaderboard based on this metric can rack up citations as people chase the top spot.
But your leaderboard is only useful if it is {\bf discriminative}: the best system reliably wins.

There are many ways questions might not be discriminative.  
If every system gets a question right (e.g., abstain on non-questions like ``asdf'' or correctly answer ``What is the capital of Poland?''), the dataset does not separate participants.  
Similarly, if every system flubs ``what is the oldest north-facing kosher restaurant'', it is not discriminative.
\newcite{sugawara-18} call these questions ``easy'' and ``hard''; we instead argue for a three-way distinction.

In between easy questions (system answers correctly with probability 1.0) and hard (probability 0.0), questions with probabilities nearer to 0.5 are more interesting.
Taking a cue from Vygotsky's proximal development theory of human learning~\cite{chaiklin-03}, these discriminative questions---rather than the easy or the hard ones---should most improve \abr{qa} systems.
These Goldilocks\footnote{In a British folktale first recorded by Robert Southey, the character Goldilocks finds three beds: one too hard, one not hard enough, and one ``just right''.} questions (not random noise) decide who tops the leaderboard.
Unfortunately, existing datasets have many easy questions.
\newcite{sugawara-20} find that ablations like shuffling word order~\cite{feng-18}, shuffling sentences, or only offering the most similar sentence do not impair systems.
Newer datasets such as \abr{drop}~\cite{dua-19} and HellaSwag~\cite{zellers-19} are harder for \emph{today}'s systems; because Goldilocks is a moving target, we propose annual evaluations in Section~\ref{sec:call}.

\subsection{Why so few Goldilocks Questions?}

This is a common problem in trivia tournaments, particularly pub quizzes~\cite{diamond-09}, where challenging questions can scare off patrons.
Many quiz masters prefer popularity with players and thus write easier questions.

Sometimes there are fewer Goldilocks questions not by choice, but by chance: a dataset becomes less discriminative through annotation error.
All datasets have some annotation error; if this annotation error is concentrated on the Goldilocks questions, the dataset will be less useful.
As we write this in 2020, humans and computers sometimes struggle on the same questions. 

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{\figfile{error_and_difficulty}}
    \caption{Two datasets with $0.16$ annotation error: the top, however, better discriminates \abr{qa} ability.  In the good dataset (top), most questions are challenging but not impossible.  In the bad dataset (bottom), there are more trivial or impossible questions \emph{and} annotation error is concentrated on the challenging, discriminative questions.  Thus, a smaller fraction of questions decide who sits atop the leaderboard, requiring a larger test set.}
    \label{fig:error-and-difficulty}
\end{figure}

Figure~\ref{fig:error-and-difficulty} shows two datsets of the same size with the same annotation error.
However, they have different difficulty \emph{distributions} and \emph{correlation} of annotation error and difficulty.
The dataset that has more discriminative questions and consistent annotator error has fewer questions that do not discriminate the winner of the leaderboard.
We call this the effective dataset proportion~$\rho$ (higher is better).
Figure~\ref{fig:how-big} shows the test set size required to reliably discriminate systems for different $\rho$, based on a simulation (Appendix~\ref{sec:synthetic-discriminative}).

At this point, you may despair about how big a dataset you need.\footnote{Using a more sophisticated simulation approach, the TREC 2002 QA test set~\cite{voorhees-03} could not discriminate systems with less than a seven absolute score point difference.}
The same terror besets trivia tournament organizers.
Instead of writing more questions, they use pyramidality (Section~\ref{sec:pyramidality}) to make every question count.


\begin{figure*}[t!]
    \begin{center}
    \includegraphics[width=1.0\linewidth]{\autofig{test_set}}
    \end{center}
    \caption{
    How much test data do you need to discriminate two systems with 95\% confidence?  This depends on both the difference in accuracy between the systems ($x$ axis) and the average accuracy of the systems (closer to 50\% is harder).  Test set creators do not have much control over those.  They do have control, however, over how many questions are discriminative.  If all questions are discriminative (right), you only need 2500 questions, but if three quarters of your questions are too easy, too hard, or have annotation errors (left), you'll need 15000.}
    \label{fig:how-big}
\end{figure*}