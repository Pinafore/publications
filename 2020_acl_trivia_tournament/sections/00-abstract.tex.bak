\begin{abstract}

Question answering (\abr{qa})is not just building systems; this \abr{nlp} subfield also creates and curates challenging question datasets that reveal the best systems.
We argue that \abr{qa} datasets---and
\abr{qa} leaderboards---closely resemble
trivia tournaments: the questions agents---humans or machines---answer reveals a ``winner''.
However, the research community has ignored the 
lessons from decades of the trivia community creating vibrant, fair,
and effective \abr{qa} competitions.
After detailing problems with existing \abr{qa} datasets, we outline several lessons that transfer to \abr{qa} research: removing ambiguity, identifying better \abr{qa} agents,
and adjudicating disputes.

\end{abstract}
