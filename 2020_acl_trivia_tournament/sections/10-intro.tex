

\section{Introduction}


In case this paper landed in in your review mixed in with more traditional \abr{acl} submissions, we want to make sure that it is clear that it is a {\bf theme paper}.
It will be long on opinions and short on experiments.
And we are going to take a somewhat unconventional analysis to try to answer the question of ``where we've been and where we're going''.
Instead of approaching the question only as \abr{acl} researchers, we're also going to try to apply some of the best practices of running a trivia tournament to whether we are doing a good job of building question answering (\abr{qa}) datasets.

The \qa{} community is obsessed with evaluation.
Schools, companies, and newspapers are obsessed with new \abr{sota}s and
topping leaderboards, e.g., claiming that topping one specific leaderboard implied that an ``\abr{ai} model tops humans''~\cite{najberg-18}, putting ``millions of jobs at risk''~\cite{cuthbertson-18}.
But what is a leaderboard? 
It is a statistic about \qa{} accuracy which then induces a ranking over participants.

Newsflash: this has the same outline as a trivia tournament (although a rather boring one compared to game shows).  
The trivia community has been doing this for decades~\cite{jennings-06}; in
Section~\ref{sec:tournament}, we argue that there's a substantial
overlap between the qualities of a first-class \abr{qa} dataset (and
its requisite paper, blog post, and leaderboard).
The trivia experts who run these tournaments are not perfect; they've
made many mistakes over the decades, but they've learned from those
mistakes to create probes to reliably judge who is best at answering
questions.
Beyond the format of the \emph{competition}, there are also important
safeguards that make sure individual questions are clear, unambiguous,
and reward knowledge (Section~\ref{sec:craft}).

We are not saying that academic \abr{qa} should surrender to trivia questions or the community---far from it!
The trivia community does not know how to ask questions that challenge computers or that resemble the information seeking needs of users on the Internet.
However, they do know how, given a bunch of questions, how to declare that one person is better at answering questions than another.
It is this collection of tradecraft and principles than in our view can help the \abr{qa} community.

Beyond these general concepts that \abr{qa} can learn from, in
Section~\ref{sec:qb} we review how these things come together into the ``gold standard'' of trivia formats and how its unnatural assumptions and conventions might help more natural \abr{qa} settings where those asking the questions are not experts.
We then briefly discuss how research that uses fun, fair, and good
trivia questions can benefit from the expertese, pedantry, and passion
of the community (Section~\ref{sec:call})---so that you too, as
researchers can benefit.