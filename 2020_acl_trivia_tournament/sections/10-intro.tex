

\section{Introduction}


This is going to be a slightly more personal introduction than usual
to a paper.
Usually, the author list is sufficient introduction and the
affiliation imparts sufficient authority to the writers.
However, in this case it is insufficient to provide the \textit{bona
  fides} necessary for this unusual aricle.
Thus, we will indulge the reader's patience by discussing the 
author's trivia experience.

Jordan Boyd-Graber has been involved in trivia since high school,
primarily through \qb{} (more on what the heck that is soon).  
He was never that good at it; his teams have only gotten as high as
fourth place (undergrad at Caltech and grad school at Princeton).
Jordan also served as an editor and director for large trivia
tournaments in five states (including founding a circuit in Colorado)
and ran the Princeton Pub quiz.
He was more sucessful as a coach; as faculty advisor, he guided
Colorado and Princeton teams to regional and national championships.
Immune to embarrassment, he also lost on the American game show \jeopardy{};  
His position on the above author list is less as a \abr{nlp} researcher as
much as an ambassador of the trivia community.


``Good for him'', we hear you say, ``I'm glad he has a hobby.  I like
roller derby, myself.  But why should the \abr{nlp} community pay any
attention to a bunch of trivia nerds?''

Instead of approaching the question only as \abr{nlp} researchers, we're also going to try to apply some of the best practices of running a trivia tournament to whether we are doing a good job of building question answering (\abr{qa}) datasets.

The \qa{} community is obsessed with evaluation.
Schools, companies, and newspapers are obsessed with new \abr{sota}s and
topping leaderboards, e.g., claiming that topping one specific leaderboard implied that an ``\abr{ai} model tops humans''~\citep{najberg-18}, putting ``millions of jobs at risk''~\cite{cuthbertson-18}.
But what is a leaderboard? 
It is a statistic about \qa{} accuracy which then induces a ranking over participants.

Newsflash: this has the same outline as a trivia tournament (although a rather boring one compared to game shows).  
The trivia community has been doing this for decades~\cite{jennings-06}; in
Section~\ref{sec:tournament}, we argue that there's a substantial
overlap between the qualities of a first-class \abr{qa} dataset (and
its requisite paper, blog post, and leaderboard).
The trivia experts who run these tournaments are not perfect; they've
made many mistakes over the decades, but they've learned from those
mistakes to create probes to reliably judge who is best at answering
questions.
Beyond the format of the \emph{competition}, there are also important
safeguards that make sure individual questions are clear, unambiguous,
and reward knowledge (Section~\ref{sec:craft}).

We are not saying that academic \abr{qa} should surrender to trivia questions or the community---far from it!
The trivia community does not know how to ask questions that challenge computers or that resemble the information seeking needs of users on the Internet.
However, they do know how, given a bunch of questions, how to declare that one person is better at answering questions than another.
It is this collection of tradecraft and principles than in our view can help the \abr{qa} community.

Beyond these general concepts that \abr{qa} can learn from, in
Section~\ref{sec:qb} we review how these things come together into the ``gold standard'' of trivia formats and how its unnatural assumptions and conventions might help more natural \abr{qa} settings where those asking the questions are not experts.
We then briefly discuss how research that uses fun, fair, and good
trivia questions can benefit from the expertese, pedantry, and passion
of the community (Section~\ref{sec:call})---so that you too, as
researchers can benefit.