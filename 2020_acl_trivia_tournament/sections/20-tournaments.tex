

\section{Surprise, You're Running a Trivia Tournament!}
\label{sec:tournament}


``But I'm doing real, important research'', you say.  
``My work is Nothing like a silly trivia tournament.''  
That may be, but let us first tell you a little about what running a tournament is like, and perhaps you might see some similarities.

This section focuses on the \abr{qa} \emph{format} broadly defined; we adopt the definition of \newcite{gardner-19}:
questions are asked by humans,
cannot be reduced to simplistic schemas,
and require unbounded natural language understanding and world knowledge. 
This section reflects the platonic ideals of what your dataset should look like if you want your metrics and leaderboard to be as effective as possible;\footnote{You may have other goals in creating a \abr{qa} dataset such as characterizing questions with factual or syntactic ambiguity.  Documenting these phenomena is valuable and we could use datasets like this!  However, such datasets may not be good candidates for the ubiquitous leaderboard.} Section~\ref{sec:craft} discusses what individual questions might look like and Section~\ref{sec:unnatural}.

First, the questions.
Either you write them yourself or you pay someone to write them (sometimes people on the internet). 
There's a fixed number of questions you need to hit by a particular date. 
We discuss the finer points of editing and writing later (Section~\ref{sec:craft}), but the first thing is that you need a bunch of questions.

Then, you advertise.  
You talk about the questions that you have (or will have).  
You talk about who is writing them, what subjects are covered, and try to get players to answer your questions.

Then you have the tournament. 
You keep your questions secure until test time, have players answer the questions, and you declare a winner. 
Afterward, people use the questions to train for future tournaments.

These have natural analogs to crowd sourcing questions, writing the paper, advertising, and running a leaderboard. 
The innate biases of academia put much more emphasis on the paper, but there are components where trivia tournament best practices could help. 
In particular, we focus on how tournaments should be fun, well-calibrated, and discriminative.

\subsection{Are we having fun yet?}
\label{sec:fun}

Many question answering papers pay crowdworkers to establish human baselines~\cite{rajpurkar-16,choi-18}.
\newcite{boyd-graber-12} instead created an interface that was fun enough that people played for free.
After two weeks the site was taken down.
However, it was popular enough that the trivia community forked the open source code to create a \href{https://protobowl.com/}{bootleg version} that is still going strong almost ten years later.

A necessary step of running a trivia tournament is a {\bf play test}.
Put yourself in the shoes of someone who has to \emph{answer} the questions.
If you find out boring, repetitive, and uninteresting, so will your crowd workers (this might interfere with measuring human performance. 
For example consider \squad{} 2.0~\cite{rajpurkar-18}, arguably the most popular computer \abr{qa} leaderboard or---in our framing---the most popular computer trivia tournament.
Many of the questions aren't particularly fun to answer from a human perspective; they're fairly formulaic.  
For example, take the question ``Along with Canada and the United Kingdom, what country generally doesn't refer to universities as private schools?''; finding the answer is equivalent to finding a list that mentions the two other countries and providing the third.
As \newcite{weissenborn-17} note, often answering these questions requires simply finding the string that matches the correct type (i.e., find some country near the phrase ``private schools'' that isn't Canada or the United Kingdom).
Other times, the nature of the task is at odds with the questions being asked.
For example, the source paragraph says ``In [Commonwealth countries]\dots the term is generally restricted to\dots Private education in North America covers the whole gamut\dots''; thus, the question ``What is the term private school restricted to in the US?'' is unanswerable not because the information is missing but because it does not appear as a span.
A human would want to be able to paraphrase and negate some of the source information to answer the question correctly.

Or consider \searchqa{}~\cite{dunn-17}, which is derived from the game \jeopardy{}, which asks ``An article that he wrote about his riverboat days was eventually expanded into \textit{Life on the Mississippi}.''
The young apprentice and newspaper writer who wrote the article is named Samuel Clemens; however, the reference answer is that author's later pen name, \underline{Mark Twain}.
Most \qa{} evaluation metrics would count \underline{Samuel Clemens} as incorrect.
In a real game of \jeopardy{}, this would not be an issue (Section~\ref{sec:ambiguity}).

Of course, fun is relative.
Many people do not find trivia fun, others travel thousands of miles to answer sports trivia questions,
and others still might have the same passion for answering math questions~\cite{amini-19}.
The deeper issues when creating a \abr{qa} task are:
have you designed a task that is internally consistent,
supported by a scoring metric that achieves your goals (more on this in a moment),
using gold annotations that correctly reward those who do the task well?
Imagine someone who loves answering the questions your task poses: would they have fun on your task?
If so, you may have a good dataset.
\newcite{ahn-06} argues that the passion of people who enjoy a task is a better (and fairer) motivator than traditional paid labeling.
Even if you pay crowdworkers, if your game is particularly unfun, you may need to think carefully about your dataset and your goals.

\subsection{Am I Measuring what I Care About?}

One way to sap the fun from a tournament is for a participant to sweat and slog through their questions to see someone who---by gaming the system---got a better score (and thus win).
While in the \abr{ml} community we might discuss this in terms of whether your metric captures the appropriate properties, but someone playing in a tournament might have a more visceral reaction: they might call your tournament unfair.
We talk about how well-written questions avoid unfairness at a low level in Section~\ref{sec:craft}, but the structure of the tournament itself might inadvertently encourage the wrong behavior (or demotivate hard-working system builders).

Answering questions requires multiple skill sets: for example, you need to know where an answer is mentioned~\cite{hermann-15}, know a cannonical name for the answer~\cite{yih-15}, known when you know the answer~\cite{rajpurkar-18}, and sometimes you need to communicate to others how to answer the question~\cite{fever-18}.
Like \abr{qa} leaderboards, trivia tournaments need to decide on a single winner.
But they also recognize that different people may have different skills.

For example, a tournament may recognize the disparate resources/preparation available at different institutions: the winner fro m high schools, small schools, universities, community colleges, or open winners~\cite{naqt-eligibility}.
Or they will give awards for specific skills (e.g., a ``golden chicken'' award for least incorrect answers, awards for specific categories, ``yo-yo'' for highest variance, or a neg award~\cite{neg-award} for most incorrect answers).
In general, the community recognizes that while a single metric will determine the winner, this is not the end of the story.

In \qa{}, the focus on \abr{sota} and leaderboards has focused attention on single metrics.
For example, in \squad{} 2.0, abstaining contributes the same to the overall \fone{} as getting an answer completely correct.
Both are clearly important, but a single score prioritizes one specific balance of the two that may not be fair.
For example in the 2018 \fever{} shared task~\cite{fever-18}, the organizers specifically devalued \fone{}, instead focusing on a metric that required just one piece of evidence per question.
The submission that had the best overall precision and \fone{} thus was in fourth place on the ``primary'' leaderboard.

\subsection{Do my questions separate the best?}
\label{sec:discriminative}

Let us assume that you have picked a metric (or a set of metrics) that capture what you care about:
systems answer questions correctly,
abstain when they cannot,
explain why they answered the way they did,
or whatever facet of \abr{qa} is most important for your dataset.
Now, you've set up a leaderboard so that you can rack up citations as people chase the leaderboard.

But if your dataset is going to be a good game (and reward those who chase the leaderboard), it should effectively decide who the winner is (again, based on the metrics that you care about).
We discuss how one format of question helps do that in Section~\ref{sec:qb} (specifically via a property called ``pyramidality''), but first let us discuss how to achieve that at a dataset level.

For computers, every system will be able to recognize that it should not attempt to answer ``asdf'', and most systems can answer questions like ``What is the capital of Poland?''.
Most systems cannot answer questions like ``What was the cause of the \abr{us} civil war?'' (perhaps nobody can).
\newcite{sugawara-18} call these questions ``easy'' and ``hard''; we would argue for a three-way distinction, however.
While easy questions might have near probability 1.0 of a system answering correctly and very hard questions might have a probability near 0.0, questions with probabilities nearer to 0.5 are more interesting.
Much like Vygotsky's theories of proximal development for human learning~\cite{chaiklin-03}---which argues that leaners are most motivated by tasks that are achievable but \emph{just} beyond their current abilities---the questions between impossible and obvious are going to best improve \abr{qa} systems.
The Goldilocks questions in between are most important for deciding who will sit atop the leaderboard; ideally these (not random noise) will decide.

Because these questions are challenging but not impossible, it is most important to get these questions right.
All datasets have some annotation error; if this annotation error is concentrated on the Goldilocks questions, the dataset will be less useful in discriminating systems on your leaderboard.

As we write this in 2019, humans and computers sometimes struggle on the same questions. 
Thus, annotation error is likely to be correlated with which questions will determine who will sit atop a leaderboard.
Particularly for test questions, this can render your dataset less useful.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{\figfile{error_and_difficulty}}
    \caption{Two datasets with $0.16$ annotation error, but the left is better at discriminating \abr{qa} ability than the right.  In the good dataset (left), most questions are challenging but not impossible.  In the bad dataset (right), there are a higher proportion of either trivial or impossible questions \emph{and} annotation error is concentrated on the challenging, discriminative questions.  Thus, there are a much higher proportion of questions that cannot decide who sits atop the leaderboard, requiring a much larger test set.}
    \label{fig:error-and-difficulty}
\end{figure}

For example, Figure~\ref{fig:error-and-difficulty} has two datsets.
They have the same annotation error and the same number of overall questions.
What is different is the distribution over difficulty levels and the correlation of annotation error and difficulty.
The dataset that has more discriminative questions and consistent annotator error has fewer questions that are effectively useless for determining the winner of the leaderboard.
We call this the effective dataset proportion~$\rho$.
A dataset with no annotation error and every question effective at discriminating teams has $\rho=1.0$, while the bad dataset in Figure~\ref{fig:error-and-difficulty} has $\rho=0.16$.

\begin{figure}[t!]
    \begin{center}
    \includegraphics[width=0.8\linewidth]{\autofig{test_set}}
    \end{center}
    \caption{
    How much test data do you need to discriminate two systems with 95\% confidence?  This depends on both the difference in accuracy between the systems ($x$ axis) and the average accuracy of the systems (closer to 50\% is harder).  As a creator of a test set, you do not have much control over those.  However, you do have control over how many questions are discriminative.  If all of your questions are discriminative (left), you need questions to discriminate systems with 1\% difference in accuracy.  But if three quarters of your questions are too easy, too hard, or have annotation errors (top), you'll need 15000 questions.}
    \label{fig:how-big}
\end{figure}

Figure~\ref{fig:how-big} shows how large a test set needs to be to discriminate systems. 
We simulate a head-to-head trivia competition where System~A and System~B have an accuracy $a$ (probability of getting a question right) separated by some difference: $a_A - a_B \equiv \Delta$.
We then simulate this on a test set of size $N$---scaled by the effective dataset proportion $\rho$---via a draw from a Bernoulli distribution,
\begin{align}
    R_a = & \mbox{Binomial}(\rho N, a_A); \notag \\
    R_b = & \mbox{Binomial}(\rho N, a_B)
    \label{eq:two-systems}
\end{align}
and see how many total test set questions (using an experiment size of 5000) are needed to detect the better system 95\% of the time (i.e., the minimum $N$ such that $R_a > R_b$ from Equation~\ref{eq:two-systems} in $0.95$ of the experiments).
Our emphasis, however is $\rho$: the smaller the percentage of discriminative questions (either because of difficulty or because of annotation error), the larger your test set must be.\footnote{Disclaimer: This should be only one of many considerations in deciding on the size of your test set.  Other factors may include balancing for demographic properties, covering linguistic variation, or capturing task-specific phenomena.}

At this point, you might be despairing about how big you need your dataset to be.
The same terror struck people who ran trivia tournaments.
We further discuss how individual questions can be made to be more discriminative using a property called pyramidality in Section~\ref{sec:pyramidality}.

