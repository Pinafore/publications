\section{The Craft of Question Writing}
\label{sec:craft}

Trivia enthusiasts agree that questions need to be well written (despite other disagreements).
Asking ``good questions'' requires sophisticated pragmatic reasoning~\cite{hawkins-15}, and pedagogy explicitly acknowledges the complexity of writing effective questions for assessing student performance~\citep[focusing on multiple choice questions]{Haladyna-04}. 

\abr{qa} datasets, however, are often collected from the wild or written by untrained
crowdworkers.
Crowdworkers lack experience in 
crafting questions and may introduce idiosyncrasies that shortcut machine learning~\citep{geva-19}.  
Similarly, data collected from the wild
such as Natural Questions~\citep{kwiatkowski-19} or AmazonQA~\cite{gupta-19} by design have vast variations in quality.
In the previous section, we focused on how datasets as a whole should be structured.
Now, we focus on how specific \emph{questions} should be structured to make the dataset as valuable as possible.


\subsection{Avoiding ambiguity and assumptions}
\label{sec:ambiguity}

Ambiguity in questions not only frustrates answerers who resolve the ambiguity `incorrectly'.
Ambiguity also frustrates the goal of using questions to assess knowledge.
Thus, the \abr{us} Department of Transportation explicitly bans ambiguous questions from exams for flight instructors~\cite{dot-08}; and the trivia community has likewise developed rules and norms that prevent ambiguity.
While this is true in many contexts, examples are rife in format called \qb{}~\cite{boyd-graber-12}, whose very long questions\footnote{Like \jeopardy{}, they are not syntactically questions but still are designed to elicit knowledge-based responses; for consistency, we still call them questions.} showcase trivia writers' tactics.
For example, \qb{} author Zhu Ying (writing for the 2005 \abr{parfait} tournament) asks participants to identify a fictional character while warning against possible confusion [emphasis added]:
\begin{quote}
 He's {\bf not Sherlock Holmes}, but his address is 221B. He's {\bf not the Janitor on Scrubs}, but his father is played by R. Lee Ermy. [\dots] For ten points, name this misanthropic, crippled, Vicodin-dependent central character of a FOX medical drama. \\
{\bf ANSWER:} Gregory \underline{House}, MD
\end{quote}

In contrast, \qa{} datasets often contain ambiguous and
under-specified questions.
While this sometimes reflects real world complexities such as actual
under-specified or ill-formed search
queries~\citep{faruqui-18,kwiatkowski-19}, ignoring this ambiguity is
problematic.
As a concrete example, Natural Questions~\cite{kwiatkowski-19} answers ``what year did the us hockey team win the Olympics'' with \underline{1960} and \underline{1980}, ignoring the \abr{us} women's team, which won in 1998 and 2018, and further assuming the query is about \emph{ice} rather than \emph{field} hockey (also an Olympic event).
Natural Questions associates a page about the United States men's national ice hockey team, arbitrarily removing the ambiguity \textit{post hoc}.
However, this does not resolve the ambiguity, which persists in the original question: information retrieval arbitrarily provides one of many interpretations.
True to their name, Natural Questions are often under-specified when users ask a question online.

The problem is neither that such questions exist nor that machine
reading \abr{qa} considers questions given an associated context.
The problem is that tasks do not explicitly acknowledge the original
ambiguity and gloss over the implicit assumptions in the data.
This introduces potential noise and bias (i.e., giving a bonus to systems that make
the same assumptions as the dataset) in leaderboard rankings. 
At best, these will become part of
the measurement error of datasets (no dataset is perfect). 
At worst, they will recapitulate the biases that went into the creation of the datasets.
Then, the community will implicitly equate the biases with correctness: you get high scores if you 
adopt this set of assumptions.
These enter into real-world systems, further perpetuating the bias.
Playtesting can reveal these issues (Section~\ref{sec:fun}), as implicit assumptions 
can rob a player of correctly answered questions.
If you wanted to answer \underline{2014} to ``when did Michigan last win the championship''---when the Michigan State Spartans won the Women's Cross Country championship---and you cannot because you chose the wrong school, the wrong sport, and the wrong gender,
you would complain as a player; researchers instead discover latent assumptions that creep into the data.\footnote{Where to draw the line is a matter of judgment; computers---which lack common sense---might find questions ambiguous where humans would not.}

It is worth emphasizing that this is not a purely hypothetical problem. For example, Open Domain Retrieval Question Answering~\cite{lee-19} deliberately avoids providing a reference context for the question in its framing but, in re-purposing data such as Natural Questions, opaquely relies on it for the gold answers.

\subsection{Avoiding superficial evaluations}

A related issue is that, in the words of \citet{voorhees-00}, ``there is no such
thing as a question with an obvious answer''.
As a consequence, trivia question authors 
delineate acceptable and unacceptable answers.

For example, in writing for the trivia tournament Harvard Fall XI, Robert Chu uses a mental model of an answerer to explicitly delineate the range of acceptable correct answers:
\begin{quote}
     In Newtonian gravity, this quantity satisfies Poisson's equation. [\dots] For a dipole, this quantity is given by negative the dipole moment dotted with the electric field. [\dots] For 10 points, name this form of energy contrasted with kinetic.\\
    {\bf ANSWER:} \underline{potential energy} \textit{(prompt on energy; accept specific types like electrical potential energy or gravitational potential energy; do not accept or prompt on just ``potential'')}
\end{quote}

Likewise, the style guides for writing questions stipulate that you
must give the answer type clearly and early on.
These mentions specify whether you want a book, a collection, a movement, etc.  
It also signals the level of specificity requested.  
For example, a question about a date must state ``day and month required'' (\underline{September 11}, ``month and year required'' (\underline{April 1968}), or ``day, month, and year required'' (\underline{September 1, 1939}).
This is true for other answers as well: city and team, party and
country, or more generally ``two answers required''.
Despite these conventions, no pre-defined set of answers is perfect,
and every worthwhile trivia competition has a process for adjudicating
answers.

In high school and college national competitions and game shows,
if low-level staff cannot resolve the issue by throwing out a single
question or accepting minor variations (\underline{America} instead of
\underline{\abr{usa}}), the low-level staff contacts the tournament
director.
The tournament director, who has a deeper knowledge of rules and questions, often decide the issue.
If not, the protest goes
through an adjudication process designed to minimize
bias:\footnote{\smallurl{https://www.naqt.com/rules/\#protest}}
write the summary of the dispute,
get all parties to agree to the summary,
and then hand the decision off to mutually agreed experts from the tournament's phone tree.
The substance of the disagreement is communicated (without identities), and the experts apply the rules and decide.

Consider what happened 
when a particularly inept \jeopardy{} contestant\footnote{\smallurl{http://www.j-archive.com/showgame.php?game_id=6112}} did not answer \underline{laproscope} to ``Your surgeon could choose to take a look inside you with this type of fiber-optic instrument''.
Since the van Doren scandal~\cite{freedman-97}, every television trivia contestant has an advocate assigned from an
auditing company.
In this case, the advocate initiated a process that went to a panel of judges who then
ruled that \underline{endoscope} (a more general term) was also correct.

The need for a similar process seems to have been well-recognized
in the earliest days of \qa{} system bake-offs such as \abr{trec-qa}, and \newcite{voorhees-08} notes that
\begin{quote}
    [d]ifferent \abr{qa} runs very seldom return exactly the same [answer], and it is quite difficult to determine automatically whether the difference [\dots] is significant.
\end{quote} 
In stark contrast to this, \qa{} datasets typically only provide a single string or, if one is lucky,
several strings.  
A correct answer means \emph{exactly} matching these strings or at least
having a high token overlap \fone{}, and failure to agree with the
pre-recorded admissible answers will put you at an uncontestable
disadvantage on the leaderboard (Section~\ref{subsection:measuring-what-you-care-about}).  

To illustrate how current evaluations fall short of meaningful
discrimination, we qualitatively analyze two near-\sota{} systems on
\squad{} V1.1: the original \xlnet{}~\citep{yang-19} and a subsequent
iteration called \xlnet{}-123.\footnote{We could not find a paper
  describing \xlnet{}-123, the submission is by
  \url{http://tia.today}.}

Despite \xlnet{}-123's margin of almost four absolute \fone{} ($94$ vs
$98$) on development data, a manual inspection of a sample of 100 of
\xlnet{}-123's wins indicate that around two-thirds are `spurious':
$56\%$ are likely to be considered not only equally good but
essentially identical; $7\%$ are cases where the answer set omits a
correct alternative; and $5\%$ of cases are `bad'
questions.\footnote{Examples in
  Appendix~\ref{sec:qualitative-analysis}.}

Our goal is not to dwell on the exact proportions, to minimize the achievements of these strong systems, or to minimize the usefulness of quantitative evaluations.
We merely want to raise the limitation of \emph{blind automation} for distinguishing between
systems on a leaderboard. 

Taking our cue from the trivia community, we present an alternative
for \abr{mrqa}.
Blind test sets are created for a specific time; all systems are submitted simultaneously.
Then, all questions and answers are revealed.
System authors can protest correctness rulings on questions, directly
addressing the issues above. 
After agreement is reached,
quantitative metrics are computed for comparison purposes---despite their inherent limitations they at least can be
trusted.
Adopting this for \abr{mrqa} would require creating a new, smaller test
set every year.
However, this would gradually refine the annotations and process.

This suggestion is not novel: \citet{voorhees-00} accept automatic
evaluations ``for experiments internal to an organization where the
benefits of a reusable test collection are most significant (\emph{and
  the limitations are likely to be understood})''~(our emphasis) but
that ``satisfactory techniques for [automatically] evaluating new
runs'' have not been found yet.  We are not aware of any change on
this front---if anything, we seem to have become more insensitive as a
community to just how limited our current evaluations are.

\subsection{Focus on the bubble}

While every question should be perfect, time and resources are limited.  
Thus, authors and editors of tournaments ``focus on the bubble'', where the ``bubble'' are the questions most likely to discriminate between top teams at the tournament.
These questions are thoroughly playtested, vetted, and edited.
Only after these questions have been perfected will the other questions undergo the same level of polish.

For computers, the same logic applies.  
Authors should ensure that these discriminative questions are correct, free of ambiguity, and unimpeachable.
However, as far as we can tell, the authors of \qa{} datasets do not give any special attention to these questions.

Unlike a human trivia tournament, however---with finite patience of the participants---this does not mean that you should necessarily remove all of the easy or hard questions from your dataset.
This could inadvertently lead to systems unable to answer simple questions like ``who is buried in Grant's tomb?''~\cite[Chapter 7]{dwan-00}.
Instead, focus more resources on the bubble.
