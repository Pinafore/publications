\section{Discussions}
We present two controlled experiments to understand how the combinations of explanation and feedback affect users' satisfaction and expectations of improvement of high and low quality ML models. We conclude that, for the simple models and task of our studies, when possible explanations and feedback should be provided together: (1) while explanations negatively impacted user satisfaction with the low quality model, they can show users how to fix models, and support for feedback had positive effects and (2) for the higher accuracy model, requesting detailed feedback without explanations reduced trust.
%participants were less frustrated with the higher accuracy model; however, requesting detailed feedback without explanations reduced trust
%, and suggested similar patterns for providing explanations without feedback (compared to no explanation); (3) therefore, models should allow for feedback if explanations are provided; 
Additionally, regardless of model quality, feature-level feedback increased expectations that models would improve, yet users generally expected model correction, regardless of whether they provided feedback or received explanations.  

%\amr{hoping to inspire future work sounds pretty full of ourselves, any better way to say this, or cut this intro sentence?}
%We hope that these studies inspire a breadth of research on the relationships between explanation and feedback in ML. 
In these experiments, we varied whether or not simple, local explanations were shown in the simple feedback mechanisms for a simple model type and task. 
Future work could explore the effects of different, more advanced, explanation types and feedback mechanisms. 
For example, we hypothesize that ``human-like'' explanations may increase expectations of improvement, as human-like characteristics in ML systems can cause users to believe systems will act rationally or take responsibility for their actions~\cite{Hook2000StepsReal}. 
%Expectations may also be affected by when explanations are shown (always or only after erring) or how users attend to them (``dismissing'' opposed to ``accepting'' or ``rejecting''). 
%Additionally, future work should explore users' experience regarding explanations and feedback for more subjective tasks and complex model types.
%Additionally, in our experiments, users were not only given methods for feedback, but required to provide it; feedback only on demand may be preferred. 
%Finally, future work might explore how users' desire for control (locus of control~\cite{Lefcourt1991LocusControl.}) impacts their desire to fix models given explanation.