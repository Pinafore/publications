\section{Data Analysis}
\label{sec:analysis}

A primary motivation for this work is to understand how different machine-in-the-loop models change the quality of entity link annotation.
For example, is it better to display only safe bets---such as links predicted by all models---or to take a ``spray and pray'' approach and display as many candidates as possible?
After this analysis, we compare summary statistics of \quel{} to other entity linking datasets.


\subsection{Dataset Statistics}
\label{sec:stats}
We compare our collected annotations to other state of the art entity linking datasets. 
We find that $\ldots$

\begin{table}
    \centering
    \small
    \begin{tabular}{lrrr}
        \toprule
        Dataset    & \textbf{\# D} & \textbf{\# T} & \textbf{\# E} \\
        \midrule
        \abr{aida} & 1393          & 301,418       & 34,929        \\
        \abr{tac}  & 0             & 0             & 0             \\
        \abr{quel} & 0             & 0             & 0             \\
        \bottomrule
    \end{tabular}
    \caption{
        Statistics of each dataset including number of \textbf{D}ocuments, \textbf{T}okens, and \textbf{E}ntities.
    }
    \label{table:gen}
\end{table}

