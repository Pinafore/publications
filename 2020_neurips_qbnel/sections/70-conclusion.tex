\section{Conclusion}
\label{sec:conc}
We introduce the new problem of noun phrase annotation, which is a generalization of \nel{}. 
We find that introducing noun phrase annotation may be useful in downstream tasks such as question answering. 
However creating a dataset for noun phrase annotation is a difficult task. 
We counter this problem by developing a human-in-the-loop method to efficiently annotate questions and motivate experts to annotate questions by assisting them with studying and directing tournaments. 
To explore the difficulty of noun phrase annotation, we propose experiments that compare the performance of \nel{} and coreference linkers on our noun phrase annotation dataset. 
We additionally design experiments to compare entity linking with multiple configurations in order to determine how best to assist users when entity linking. 
We finally design experiments to determine the effect that noun phrase annotation has upon question answering; we do this by comparing the accuracy of QA models when replacing entities with their entry in the knowledge base. 
Our next steps are to proceed forward with data collection, and to run the experiments on the noun phrase dataset. 