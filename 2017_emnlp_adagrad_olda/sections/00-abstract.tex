\begin{abstract}

  Online topic modeling, i.e., topic modeling with stochastic variational inference, is a powerful and efficient technique for analyzing large datasets,
  and \adagrad{} is a widely-used technique for tuning learning rates
  during online gradient optimization.  However, these two techniques
  do not work well together. We show that this is because
  \abr{adagrad} uses accumulation of previous gradients as the
  learning rates' denominators. For online topic modeling, the
  magnitude of gradients is very large. It causes learning rates to shrink very quickly, so the parameters cannot fully converge until the training ends.


\end{abstract}



\label{sec:abstract}
