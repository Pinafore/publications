% conclusion
\begin{frame}{Summary}
\begin{enumerate}
\item Neural CR models cannot immediately adapt without training on in-domain,
labeled data
\item To reduce amount of annotation, we use active learning to choose particular text spans for users to label
\item We explore various aspects of active learning for CR, including sources of
model uncertainty and the trade-off between reading and labeling
\item Sampling by mention detection entropy is more useful for domains like
PreCo
\item In both simulations and the user study,
CR improves from continued training on spans sampled from the same document rather than different
contexts
\end{enumerate}
\end{frame}
