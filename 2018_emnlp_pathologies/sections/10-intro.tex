\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
\small
\tikz\node[fill=white!90!colorsquad,inner sep=1pt,rounded corners=0.3cm]{
\begin{tabular}{lp{0.7\columnwidth}}
\textbf{\squad{}} \\
    Context & In 1899, John Jacob Astor IV invested \$100,000 for Tesla to further
develop and produce a new lighting system. Instead, Tesla used the
money to fund his \mybox{coloranswer}{Colorado Springs experiments}. \\\\
Original & What did Tesla spend Astor's money on ? \\
Reduced & did  \\
Confidence & 0.78 $\to$ 0.91 \\
\end{tabular}
}; 
\caption{\squad{} example from the validation set. Given the original
    \emph{Context}, the model makes the same correct prediction (``Colorado
    Springs experiments'') on the \emph{Reduced} question as the
    \emph{Original}, with even higher confidence. For humans, the reduced
    question, ``did'', is nonsensical.}
\label{fig:intro_example}
\end{figure}

Many interpretation methods for neural networks explain the model's prediction
as a counterfactual: how does the prediction
change when the input is modified?  Adversarial
examples~\cite{szegedy2013intriguing,goodfellow2014explaining} highlight the
instability of neural network predictions by showing how small 
perturbations to the input dramatically change the output.

A common, non-adversarial form of model interpretation is feature attribution:
features that are crucial for predictions are highlighted in a heatmap. One can
measure a feature's importance by input perturbation. Given an input for text
classification, a word's importance can be measured by the difference in model
confidence before and after that word is removed from the input---the word is
important if confidence decreases significantly. This is the \loo{}
method~\cite{li2016understanding}.
Gradients can also reveal feature importance: a feature
influences predictions if its gradient is a large
positive value. Both perturbation and gradient-based methods can
generate heatmaps, implying that the model's prediction is highly
influenced by the highlighted, important words.

Instead, we study how the model's prediction is influenced by 
\emph{unimportant} words.  We use \textbf{input reduction}, iteratively removing unimportant words from the input while maintaining the
model's prediction.  Intuitively, the words remaining after input reduction should
be important for prediction.  Moreover, the words should match the \loo{}
method's selections, which closely align with human
perception~\cite{li2016understanding,murdoch2018cd}.  However, rather than
providing explanations of the original prediction, our reduced examples more
closely resemble adversarial examples. The
reduced input is meaningless to a human but retains the same model prediction
with high confidence (Figure~\ref{fig:intro_example}). Gradient-based input reduction exposes pathological model
behaviors that contradict what one expects based on existing interpretation
methods.

In Section~\ref{sec:method}, we construct these counterintuitive
examples by augmenting input reduction with beam search and experiment
with three tasks: \squad~\cite{rajpurkar2016squad} for reading
comprehension, \snli~\cite{bowman2015snli} for textual entailment, and
\vqa~\cite{antol2015vqa} for visual question answering.  Input
reduction with beam search consistently reduces the input sentence to
very short lengths---often only one or two words---without lowering
model confidence on its original prediction.  The reduced examples
appear nonsensical to humans, which we verify with crowdsourced
experiments. In Section~\ref{sec:confidence}, we draw connections to
adversarial examples and confidence calibration; we explain why the
observed pathologies are a consequence of the overconfidence of neural
models. This elucidates limitations of interpretation methods that
rely on model confidence. In Section~\ref{sec:entropy_training}, we
encourage high model uncertainty on reduced examples with entropy
regularization. The pathological model behavior under input reduction
is mitigated, leading to more reasonable reduced examples.
