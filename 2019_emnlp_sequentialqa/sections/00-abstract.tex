Question answering is an \abr{ai}-complete problem, but existing datasets
lack key elements of language understanding such as coreference and
ellipsis resolution.
We consider sequential question answering: multiple questions are
asked one-by-one in a conversation between a questioner
and an answerer. 
Answering these questions is only possible through understanding the
conversation history.
We introduce the task of question-in-context
rewriting: given the context of a conversation's history, rewrite a
context-dependent into a self-contained
question with the same answer.
We construct, \name{}, a dataset of 40,527 questions based on
\quac~\cite{ChoiQuAC2018} and train Seq2Seq models for incorporating context into standalone questions.


