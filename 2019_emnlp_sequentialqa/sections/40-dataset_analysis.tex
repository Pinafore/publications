
\section{Dataset and Model Analysis}

\label{sec:analysis}

We analyze our dataset with automatic metrics after validating the reliability of our data (Section~\ref{sec:data}).
We compare our dataset to the original \abr{\quac} questions and to automatically generated questions by our models. Then, we manually inspect the sources of rewriting errors in the seq2seq baseline.

\subsection{Anaphora Resolution and Coreference}
Our rewrites are longer, contain more nouns and less pronouns, and have more  word types than the original data. 
Machine output lies in between the two human-generated corpora, but quality is difficult to assess. 
Figure~\ref{fig:bargraph} shows these statistics.  
We motivate our rewrites by exploring linguistic properties of our
data. 
Anaphora resolution and coreference are two core \textsc{nlp} tasks applicable to this dataset, in addition to the downstream tasks evaluated in Section~\ref{sec:expr}.  




\begin{figure}[t!]
  \centering
	\includegraphics[width=\linewidth]{2019_emnlp_sequentialqa/auto_fig/length_and_ratio}
	\caption{Human rewrites are longer, have fewer pronouns, and
          have more proper nouns than the original \abr{\quac} questions.
Rewrites are longer and contain more proper nouns than our Pronoun Sub
baseline and trained Seq2Seq model.}
	\label{fig:bargraph}
\end{figure}

    
Pronouns occur in 53.9\% of \abr{\quac} questions.  Questions with pronouns are more likely to be ambiguous than those without any.   
 Only 0.9\% of these have pronouns that span more than one category
 (e.g., `she' and `his').  Hence, pronouns within a single sentence are likely unambiguous.  
 However, 75.0\% of the aggregate history has pronouns and the percentage of mixed category pronouns increase to 27.8\% of our data.  Therefore, pronoun disambiguation potentially becomes a problem for a quarter of the original data.  An example is provided in Table~\ref{tab:coreferenceexample}.


 
\begin{table}
	\small
	\begin{tabular*}{\linewidth}{l p{5cm}}
          		\toprule
		{\bf Label} & {\bf Text} \\
          \hline
          QUESTION & How long did he stay there? \\
          \rowcolor{gray!25}
		REWRITE  & How long did Cito Gaston stay at the Jays? \\
		HISTORY & \parbox{5 cm}{\textit{Cito Gaston}
                           \newline  {\bf Q:} What did Gaston do after the world series?\newline \dots \newline {\bf Q:} Where did he go in 2001?  \newline {\bf A:} In 2002, he was hired by the Jays as special assistant to president and chief executive officer Paul Godfrey.} \\
		\bottomrule
	\end{tabular*}
	\caption{An example that had over ten flagged proper nouns in the history. Rewriting requires resolving challenging coreferences.}
	  
	\label{tab:coreferenceexample}
\end{table}



Approximately one-third of the questions generated by our
pronoun-replacement baseline are within 85\% string similarity to our
rewritten questions.
That leaves two-thirds of our data that cannot be solved with pronoun resolution alone. 


\subsection{Model Analysis}
\label{sec:models}
\begin{table*}
\centering
\begin{tabular}{lp{6cm}p{8.5cm}}
	  \toprule
	 & \textbf{Seq2Seq output } & \textbf{Reference}\\
  \hline
1 & What did Chamberlain's men do? & What did Chamberlain's men do during the Battle of Gettysburg? \\
  2 & How many games did Ozzie Smith win? & How many games did the Cardinals win while Ozzie Smith played? \\
  3 & Did 108th get to the finals? & Did the US Women's Soccer Team get to the finals in the 1999 World Cup? \\
  4 & Did Gabriel Batistuta reside in any other countries, besides touring in the Copa America? & Besides Argentina, did Gabriel Batistuta reside in any other countries? \\
5 & Did La Comedia have any more works than La Comedia 3? & Did Giannina Braschi have any more works than United States of Banana, La Comedia and Asalto al tiempo? \\
\bottomrule
\end{tabular}
\caption{Example erroneous rewrites generated by the Seq2Seq models
and their corresponding reference rewrites. The dominant
source of error is the model tendency to produce
short rewrites (Examples 1--3). Related entities (Copa America and Argentina in Example 4) distract the model. The model
struggles with listing multiple entities mentioned in different
parts of the context (Example 5).
}
  \label{tab:erranalysis}
\end{table*}


By manually examining the predictions of the seq2seq model, we notice that the main source of errors is that the model tends to find a short path to completing the rewrites. That often results in \textit{under-specified questions} as in Example~1 in Table~\ref{tab:erranalysis}, \textit{question meaning change} as in Example~2 or \textit{meaningless questions} as in Example~3.

Another source of errors is having related entities mentioned in the context as 
Example~4 in Table~\ref{tab:erranalysis}, where the model confused ``Copa America''
with ``Argentina''. The model also struggles with listing multiple entities mentioned in different parts of the context. Example~5 in Table~\ref{tab:erranalysis}
show the output and the reference rewrites of the question \textit{``Did she have any more works than those 3?''}, where two of the three
	entities---``United States of Banana'', ``La Comedia'' and ``Asalto al tiempo''---are lost in the rewrite.  





