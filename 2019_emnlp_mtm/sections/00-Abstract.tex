\begin{abstract}
  Multilingual topic models (\mtms) learn topics on documents in
  multiple languages.
  Past models align topics across languages by implicitly assuming the
  documents in different languages are highly comparable, often a false
  assumption.
  We introduce a new model that does not rely on this assumption,
  particularly useful in important low-resource language scenarios.
  Our \mtm learns weighted topic links and connects cross-lingual
  topics only when the dominant words defining them are similar,
  outperforming \lda and previous \mtms in classification tasks using
  documents' topic posteriors as features.
  It also learns coherent topics on documents with low
  comparability.

\jbgcomment{Coming back to Philip's earlier points, if we wanted to
  change title to remove ``incomparable'', now we're able to. \wycomment{Done.}}

\end{abstract}
