Leaderboards are widely used in \nlp{} and push the field forward.
While leaderboards are a straightforward ranking of \nlp{} models, this simplicity can mask nuances in evaluation \itms{} (examples) and \subjs{} (\nlp{} models).
Rather than replace leaderboards, we advocate a re-imagining so that they better highlight if and where progress is made.
Building on educational testing, we create a Bayesian leaderboard model where latent \subj{} skill and latent \itm{} difficulty predict correct responses.
Using this model, we analyze the ranking reliability of leaderboards.
Afterwards, we show the model can guide what to annotate, identify annotation errors, detect overfitting, and identify informative examples.
We conclude with recommendations for future benchmark tasks.
