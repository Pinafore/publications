\section{Conclusion}
\label{sec:conclusion}


We have provided a comprehensive analysis of topic model evaluation in multilingual settings,
including for low-resource languages.  While evaluation is an important area of topic model research,
no previous work has studied evaluation of multilingual topic models.
Our work provided two primary contributions to this area, including
a new intrinsic evaluation metric, \cnpmi{},
as well as a model for adapting this metric to low-resource languages without large reference corpora.





As the first study on evaluation for multilingual topic models,
there is still room for improvement and further applications. 
For example, human judgment is more difficult to measure than in monolingual settings,
and it is still an open question on how to design a reliable and accurate survey for multilingual quality judgments. 
As a measurement of multilingual coherence,
we plan to extend \cnpmi{} to high-dimensional representations,
\textit{e.g.,} multilingual word embeddings,
particularly in low-resource languages~\cite{Ruder17}.



