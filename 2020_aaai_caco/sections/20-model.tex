\section{\name{}: Classification Aided by Convergent Orthography}
\label{sec:model}

This section introduces our method, \name{}, which trains a multilingual
document classifier using labeled datasets in a source language $\mathcal{S}$
and applies the classifier to a low-resource target language $\mathcal{T}$.
We focus on the setting where $\mathcal{S}$ and $\mathcal{T}$ are related and
have similar orthographic features.

\subsection{Model Architecture}


Let $\vect{x}$ be an input document with a sequence of words $\vect{x} =
\langle \vect{w}_1, \vect{w}_2, \cdots, \vect{w}_n \rangle$, where each word
$\vect{w}_i$ is a sequence of character.
Our model maps the document~$\vect{x}$ to a distribution over possible
labels~$y$ in two steps (Figure~\ref{fig:model}).
First, we generate a word embedding~$\vect{v}_i$ for each input
word~$\vect{w}_i$ using a character-based embedder~$e$:
\begin{equation}
  \vect{v}_i = e(\vect{w}_i).
\end{equation}
We then feed the word embeddings to a word-based classifier~$f$ to compute the
distribution over labels $y$:
\begin{equation}
  p(y \g \vect{w}) = f\left( \langle \vect{v}_1, \vect{v}_2, \cdots, \vect{v}_n \rangle \right ).
\end{equation}
We can use any sequence model for the embedder~$e$ and the classifier~$f$.  For
our experiments, we use a bidirectional \abr{lstm}
~\citep[\abr{bi-lstm}]{graves-05} embedder and a deep averaging network
~\citep[\abr{dan}]{iyyer-15-fixed} classifier.

\paragraph{\abr{bi-lstm} Embedder.}
\abr{bi-lstm} is a powerful sequence model that captures complex non-local
dependencies.  Character-based \abr{bi-lstm} embedders are used in
many natural language processing
tasks~\citep{ling-15a,ballesteros-15,lample-16}.
To embed a word $\vect{w}$, we pass the character sequence~$\vect{w}$ to a
left-to-right \abr{lstm} and the reversed character sequence~$\vect{w'}$ to a
right-to-left \abr{lstm}.  We concatenate the final hidden states of the two
\abr{lstm} and apply a linear transformation:
\begin{equation}
  e(\vect{w}) = \vect{W}_e \cdot [\overrightarrow{\text{\abr{lstm}}}(\vect{w});
  \overleftarrow{\text{\abr{lstm}}}(\vect{w'})] + \vect{b}_e,
\end{equation}
where the functions $\overrightarrow{\text{\abr{lstm}}}$ and
$\overleftarrow{\abr{lstm}}$ compute the final hidden states of the two
\abr{lstm}s.

\paragraph{\abr{dan} Classifier.}
A \abr{dan} is an unordered model that passes the arithmetic mean of the input
word embeddings through a multilayer perceptron and feeds the final layer's
representation to a softmax layer.
\abr{dan} ignores cross-lingual variations in word order (i.e., syntax) and
thus generalizes well in \abr{cldc}.
Despite its simplicity, \abr{dan} has near state-of-the-art accuracies on both
monolingual~\citep{iyyer-15-fixed} and cross-lingual document
classification~\citep{chen-18}.

Let  $\vect{v}_1,\vect{v}_2,\cdots,\vect{v}_n$ be the word embeddings
generated by the character-based embedder.
\abr{dan} uses the average of the word embeddings as the document
representation $\vect{z}_0$:
\begin{equation}
  \vect{z}_0 = \frac{1}{n}\sum_{i=1}^n \vect{v}_i,
\end{equation}
and $\vect{z}_0$ is passed through $k$ layers of non-linearity:
\begin{equation}
  \vect{z}_i = g(\vect{W}_i \cdot \vect{z}_{i-1} + \vect{b}_i),
\end{equation}
where $i$ ranges from 1 to $k$, and $g$ is a non-linear activation
function.
The final representation~$\vect{z}_{k}$ is passed to a softmax layer to obtain
a distribution over the label $y$,
\begin{equation}
    p(y \g \vect{x}) = \text{softmax}(\vect{W}_{k+1} \vect{z}_k + \vect{b}_{k+1}).
\end{equation}

We use the same classifier parameters $\vect{W}_i$ across languages.
In other words, the \abr{dan} classifier is language-independent.
This is possible because the embedder generates consistent word representations
across related languages, which we discuss in the next section.

\subsection{Character-Level Cross-Lingual Transfer}
\label{ssec:char}

To transfer character-level information across languages, the embedder uses
the same character embeddings for both languages.
The character-level \abr{bi-lstm} vocabulary is the union of the alphabets for
the two languages, and the embedder does not differentiate identical characters
from different languages.
For example, a Spanish ``a'' has the same character embedding as a French
``a''.
Consequently, the embedder maps words with similar forms from both languages to
similar vectors.

If the source language and the target language are orthographically similar, the
embedder can generalize knowledge learned about source language words to target
language words through shared orthographic features.  As an example, if the
model learns that the Spanish word ``religioso'' (religious) is predictive of
label~$y$, the model automatically infers that ``religioso'' in Italian is also
predictive of $y$, even though the model never sees any Italian text.

In our experiments, we focus on related language pairs that share the same
script.
For related languages with different scripts, we can apply \name{} to the
output of a transliteration tool or a grapheme-to-phoneme
transducer~\citep{mortensen-18}.
We leave this to future work.

\subsection{Training Objective}\label{ssec:objective}

Our main objective is supervised document classification.  We jointly
train the classifier and the embedder to minimize average negative
log-likelihood on labeled source language documents $S$:
\begin{equation}
  L_s(\vect{\vect{\theta}}) = -\frac{1}{|S|}\sum_{\langle \vect{x}, y \rangle}
  \log p(y \g \vect{x}),
\end{equation}
where $\vect{\theta}$ is a vector representing all model parameters, and $S$ is
a set of source language examples with words~$\vect{x}$ and label~$y$.

Sometimes we have additional resources for the source or target language.  We
use them to improve \name{} with multi-task learning~\citep{collobert-11b}
via three auxiliary tasks.  

\paragraph{Word Translation (\abr{dict}).}
There are many patterns when translating cognate words between related
languages.
For example, Italian ``e'' often becomes ``ie'' in Spanish.
``Tempo'' (time) in Italian becomes ``tiempo'' in Spanish, and ``concerto''
(concert) in Italian becomes ``concierto'' in Spanish.
The embedder can learn these word translation patterns from a bilingual
dictionary.

Let $D$ be a bilingual dictionary with a set of word pairs $\langle \vect{w}_s,
\vect{w}_t \rangle$, where $\vect{w}_s$ and $\vect{w}_t$ are translations of
each other. 
We add a term to our objective to minimize average squared Euclidean distances
between the embeddings of translation pairs~\citep{mikolov-13b}:
\begin{equation}
  L_d(\vect{\theta}) = \frac{1}{|D|}\sum_{\langle \vect{w}_s, \vect{w}_t \rangle}
  \norm{e(\vect{w}_s) - e(\vect{w}_t)}_2^2.
\end{equation}

\paragraph{Mimicking Word Embeddings (\abr{mim}).}
Monolingual text classifiers often benefit from initializing embeddings with
word vectors pre-trained on large unlabeled corpus~\citep{collobert-11b}.  This
semi-supervised learning strategy helps the model generalize to word types
outside labeled training data.  Similarly, our embedder can
\abr{mimick}~\citep{pinter-17} an existing \emph{source language}
word embeddings to generalize better.

Suppose we have a pre-trained source language word embedding matrix $\vect{E}$
with $V$ rows.  The $i$-th row $\vect{x}_i$ is a vector for the $i$-th word
type $\vect{w}_i$.
We add an objective to minimize the average squared Euclidean distances between
the output of the embedder and $\vect{E}$:
\begin{equation}
  L_e(\vect{\theta}) = \frac{1}{V} \sum_{i=1}^V \norm{e(\vect{w}_i) - \vect{E}_i}_2^2.
\end{equation}

\begin{table*}
  \centering
  \begin{tabular}{lccccccc}
    \toprule
      & \multicolumn{4}{c}{\name{}} & \multicolumn{2}{c}{Baseline}\\
    \cmidrule(lr){2-5} \cmidrule(lr){6-7}
    & \abr{src} & \abr{dict} & \abr{mim} & \abr{all} & \abr{clwe} & \abr{sup} & \abr{com}\\
    \midrule
    Source labeled data & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark\\
    Pre-trained source embedding & & & \checkmark & \checkmark \\
    Small dictionary & & \checkmark & & \checkmark \\
    Pre-trained \abr{clwe} & & & & & \checkmark & & \checkmark\\
    Target labeled data & & & & & & \checkmark\\
    \midrule
    \abr{rcv2} average accuracy & 50.0 & 55.7 & 51.5 & 54.7 & 51.6 & 51.9 & \textbf{64.5} \\
    \bottomrule
  \end{tabular}
  \caption{Comparison of models used in our experiments (introduced in
  Section~\ref{ssec:model}).
  For each model, we list its required resources and average accuracy on
  \abr{rcv2} over eight related language pairs (accuracy for each pair in
  Table~\ref{tab:rcv2}).
  We compare \name{} variants with two high-resource models: a \abr{clwe}-based
  model (\abr{clwe}) and a lightly supervised target language model
  (\abr{sup}).
  Both baselines require more target language resources than \name{} variants,
  and yet they have lower average accuracy than some \name{} variants, which
  confirms that character-level knowledge transfer is highly efficient.
  We also experiment with a model that combines \abr{clwe} with \name{} (\abr{com}).
  This combined model has the highest average accuracy, indicating that
  \abr{clwe} and \name{} are complementary when both options are available.
  } 
  \label{tab:model}
\end{table*}

\paragraph{Knowledge Distillation.}
Sometimes we have a reliable reference classifier in another high-resource
language $\mathcal{H}$ (e.g., English).  If we have parallel text
between $\mathcal{S}$ and $\mathcal{H}$, we can use knowledge
distillation~\citep{xu-17} to supply additional training signal.  Let $P$
be a set of parallel documents $\langle \vect{x}_s, \vect{x}_h \rangle$, where
$\vect{x}_s$ is from source language $\mathcal{S}$, and $\vect{x}_h$ is the
translation of $\vect{x}_s$ in $\mathcal{H}$.  We add another objective term to
minimize the average Kullback-Leibler divergence between the predictions of our
model and the reference model:
\begin{equation}
  L_p(\vect{\theta}) = \frac{1}{|P|} \sum_{\langle \vect{x}_s, \vect{x}_h \rangle \in P}
  \kldiv{p_h (y \g \vect{x}_h)}{p(y \g \vect{x}_s)},
\end{equation}
where $p_h$ is the output of the reference classifier (in language
$\mathcal{H})$, and $p$ is the output of \name{}.  In \S~\ref{sec:experiments},
we mark models that use knowledge distillation with a superscript~``\abr{p}''.

We train on the four tasks jointly.  Our final objective is:
\begin{equation}
  L(\vect{\theta}) = L_s(\vect{\theta}) + \lambda_d L_d(\vect{\theta}) + \lambda_e L_e(\vect{\theta}) + \lambda_p L_p(\vect{\theta}),\label{eq:full_obj}
\end{equation}
where the hyperparameters $\lambda_d$, $\lambda_e$, and $\lambda_p$ trade off
between the four tasks.
