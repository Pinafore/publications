\section{Related Work}\label{sec:related}

Previous \abr{cldc} methods are typically word-based and rely on one of the
following cross-lingual signals to transfer knowledge: large bilingual
lexicons~\citep{shi-10,andrade-15}, \abr{mt} systems
~\citep{banea-08-fixed,wan-09-fixed,zhou-16}, or
\abr{clwe}~\citep{klementiev-12}.
One exception is the recently proposed multilingual
BERT~\citep{devlin-19,wu-19}, which uses a subword vocabulary.
Unfortunately, some languages do not have these resources.
\name{} can help bridge the resource gap.
By exploiting character-level similarities between related languages, \name{}
can work effectively with few or no target language data.

To adapt \abr{clwe} to low-resource settings, recent unsupervised \abr{clwe}
methods~\citep{conneau-18,artetxe-18b} do not use dictionary or parallel text.
These methods can be further improved with careful
normalization~\citep{zhang-19} and interactive refinement~\citep{yuan-19}.
However, unsupervised \abr{clwe} methods still require large monolingual
corpora in the target language, and they might fail when the monolingual corpora
of the two languages come from different domains~\citep{sogaard-18,fujinuma-19}
and when the two language have different morphology~\citep{czarnowska-19}.
In contrast, \name{} does not require any target language data.

Cross-lingual transfer at character-level is successfully used in low-resource
paradigm completion~\citep{kann-17}, morphological
tagging~\citep{cotterell-17a}, part-of-speech tagging~\citep{kim-17}, and named
entity recognition~\citep{bharadwaj-16,cotterell-17b,lin-18,rijhwani-19}, where
the authors train a character-level model jointly on a small labeled corpus in
target language and a large labeled corpus in source language.
Our method is similar in spirit, but we focus on \abr{cldc}, where it is less
obvious if orthographic features are helpful.
Moreover, we introduce a novel multi-task objective to use different types of
monolingual and cross-lingual resources.

\begin{table}
  \tabcolsep=0.15cm
  \centering
  \begin{tabular}{llcccc}
    \toprule
    source & target & \abr{src} & \abr{dict} & \abr{mim} & \abr{all} \\
    \midrule
    \flag{fr}~\flag{it}~\abr{fr}/\abr{it} & \flag{es}~\abr{es} & 58.8 & 67.0 & 55.8 & 65.3 \\
    \flag{es}~\flag{it}~\abr{es}/\abr{it} & \flag{fr}~\abr{fr} & 51.8 & 55.8 & 50.3 & 56.0 \\
    \flag{es}~\flag{fr}~\abr{es}/\abr{fr} & \flag{it}~\abr{it} & 53.2 & 56.1 & 55.9 & 56.5 \\
    \multicolumn{2}{r}{average} & 54.6 & 59.6 & 54.0 & 59.3 \\
    \bottomrule
  \end{tabular}
  \caption{Results of \abr{cldc} experiments using two source languages.
  Models trained on two source languages are generally better than models
  trained on only one source language (Table~\ref{tab:rcv2}).}
  \label{tab:multisrc} 
\end{table}

\begin{table}
  \centering
  \begin{tabular}{llRR}
    \toprule
    source & target & \abr{clwe} & \name{}\\
    \midrule
    \flag{es}~\abr{es} & \flag{fr}~\abr{fr} & 36.8 & 31.1\\
    \flag{es}~\abr{es} & \flag{it}~\abr{it} & 44.0 & 33.1\\
    \flag{fr}~\abr{fr} & \flag{es}~\abr{es} & 34.0 & 30.9\\
    \flag{fr}~\abr{fr} & \flag{it}~\abr{it} & 33.5 & 29.6\\
    \flag{it}~\abr{it} & \flag{es}~\abr{es} & 42.1 & 37.5\\
    \flag{it}~\abr{it} & \flag{fr}~\abr{fr} & 35.6 & 36.4\\
    \multicolumn{2}{r}{average} & 37.7 & 33.1\\
    \bottomrule
  \end{tabular}
  \caption{Word translation accuracies (P@1) for different embeddings.  The
  \name{} embeddings are generated by the embedder of a \abr{src} model trained
  on the source language.  Without any cross-lingual signal, the \name{}
  embedder has competitive word translation accuracy as \abr{clwe} pre-trained
  on large target language corpora and dictionaries.}
  \label{tab:bli} 
\end{table}
