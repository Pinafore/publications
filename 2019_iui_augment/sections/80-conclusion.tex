\section{Conclusion}
\label{sec:conclusion}

We propose and demonstrate an evaluation of interpretation methods in a
human-\abr{ai} cooperative setting. We focus on the natural language
domain and use a question answering task derived from a popular trivia
game, \qb{}.
Our experiments with both experts and novices reveal how they trust and
use interpretations differently, producing a more accurate and
realistic evaluation of machine learning interpretability. Our results
highlight the importance of taking the skill level of the target user
into consideration, and suggests that, combining
interpretations more intelligently and adapting to the user, we can
further improve the human-\abr{ai} cooperation.
