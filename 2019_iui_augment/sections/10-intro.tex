\section{Introduction}
\label{sec:intro}

% 1. ML is good, but still need humans
The field of machine learning (\abr{ml}) is making rapid progress,
with models surpassing human performance on many tasks, such as
image classification~\cite{he2015delving}, playing video
games~\cite{mnih2015human}, and playing Go~\cite{silver2017mastering}.
However, a drop-in replacement for humans---even assuming that it is
achievable---is not always the ideal integration of machine learning
into real-world decision making.
In sensitive areas such as medicine and criminal justice, the
computational objectives of \abr{ml} models cannot yet fully
capture the factors one must consider when making a
decision, such as fairness and transparency. In some other areas such
as natural language processing, the
strengths of humans and computers are sometimes complimentary.
Humans are excellent at reasoning about what we
consider ``common sense'', while some tasks in this category such as
disambiguating word senses are still difficult for
computers~\cite{papandrea2017supwsd}.
Tasks like deceptive review detection is difficult and time consuming
for humans while simple linear \abr{ml} models achieve high accuracy
with little processing time~\cite{lai2018human}.
On tasks such as simultaneous interpretation where humans are
still far superior than computers, experts can still be assisted on
some aspects of the task: interpreters often
find certain content such as technical terms, names of people and
organizations, and numbers difficult to translate, while 
computers find that easy.  The integration of \abr{ml}
can be more effective and efficient when humans and computers
cooperate.

% 2. the need for interpretation in cooperation
Cooperation is only effective when the two parties communicate
well with each other. One direction of this communication, from
humans to computers, is well-studied:
\abr{ml} models can be improved with human feedback using
reinforcement learning~\cite{sutton1998introduction} and imitation
learning~\cite{ross2011reduction,ross2018regularizing}. The
other direction of the communication, from \abr{ml} models to
humans, presents different challenges:
a standard classification model outputs a prediction (e.g., an object
class given an image), but without any justification.
% It is often not insightful to look directly at the model's
% internals, which in the case of modern neural networks consist of
% millions of parameters.
Although the prediction can be presented with a confidence score (a
value between zero and one), humans struggle to interpret and act on
numbers~\cite{peters2006numeracy, reyna2008numeracy}; moreover, due to
over-fitting, confidence scores from a neural models can be much
higher than the actual prediction
uncertainty~\cite{guo2017calibration}.

% 3. the role of interpretation
To bridge the gap between human and \abr{ml} models in a cooperative
setting, interpretation methods 
explain the model predictions in a more expressive,
human-intelligible way. In a human-centered setting where humans make
the final decision, these methods help users decide to trust the
model prediction or not. In Section~\ref{sec:related} we discuss the
existing work of interpreting \abr{ml} models.

% 4. evaluation of interpretation is difficult
Progress in \abr{ml} research largely relies on rigorous evaluations,
which often relies on standard
datasets, for example ImageNet~\cite{deng2009imagenet} for image
classification and Penn Treebank~\cite{marcus1993building} for
language modeling.
Although interpretability is valued as a laudable goal, it remains
elusive to evaluate.
We do not have such standard dataset for interpretability---it is not
clear what the ground truth should be. As
Lipton~\cite{lipton2016mythos} argues, there is no clear agreement on what
interpretability means; there is no definitive answer to what
interpretation is most faithful to the model and useful for humans at
the same time. Secondly, it is not realistic to evaluate
interpretability without humans, the eventual consumer of
interpretations~\cite{narayanan2018humans}.
Previous work focuses on how humans can use interpretations to help
the model do its job better; for example, interpretations generated by
Local Interpretable Model-Agnostic
Explanations~\cite[\abr{lime}]{ribeiro2016lime} help humans do feature
engineering to improve downstream predictions of a classifier; in
other work interpretations are used to help humans debug \abr{ml}
models~\cite{ribeiro2018semantically,fong2017interpretable}.

% 5. we take the human-centered perspective
Kleinberg~\etal{}~\cite{kleinberg2017human} propose a different
perspective and ask how \abr{ml} can improve human decision making.
Applying this thinking, we measure
interpretability by asking what \abr{ml} can do
for humans through interpretations: they should
\emph{augment}~\cite{Koedinger-13} human intelligence. This concept
resonates with the seminal work of mixed-initiative user
interface~\cite{horvitz1999principles}, which emphasizes 
user interfaces where the human and the computer can drive towards a
shared goal and ones that enhance human ability~\cite{allen1999mixed}.

% 6. why we focus on forms instead of specific methods
Interpretations come in many forms; we focus on three
popular options among the interpretable \abr{ml} community:
visualizing uncertainty, highlighting
important input features, and retrieving relevant training
examples.  We measure how they help humans on the tasks at hand and
focus on answering the question ``how effective can interpretations
communicate model predictions to humans''. The other question is ``how
faithful an interpretation is to the model''.
Section~\ref{sec:eval_qb} discusses our choice of model to answer the
first question; we leave the second question to future work, but
discuss in Section~\ref{sec:discussion} how our framework, interface,
and experiments can be directly applied.

% 7. introduce QB, forward point qb section
We choose the testbed for our interpretability evaluation from the
natural language domain---a question answering task called
\qb{}~\cite{boydgraber2012besting}.  As we discuss in
Section~\ref{sec:eval_qb}, in addition to being a
challenging task for \abr{ml}, it is also an exciting game that is
loved by human trivia enthusiasts. Furthermore, it is a task where
humans and \abr{ml} have complementary strengths, so effective
collaboration with interpretations has great potential.

% 8. a teaser of what we do
We recruit both \qb{} enthusiasts and turkers from Amazon Mechanical
Turk (novices in comparison) to play \qb{} on an interactive
interface,
provide them different combinations of the interpretations, and
measure how their performance changes.  These different user groups
reveal imperfections in how we communicate the way a computer answers
questions.  Experts have enough world and task expertise to
confidently overrule when the computer is wrong; however, as we will
discuss in Section~\ref{sec:results}, novices are too trusting: they
play more aggressively with computer assistance, but are not able to
discern useful help from the misleading ones as well as the experts.
In Section~\ref{sec:discussion}, we propose how to can explore new
interpretations and visualizations to help humans more confidently
interpret \abr{ml} algorithms.
