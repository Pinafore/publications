% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

% indirect injection
@article{greshake2023youve,
    title={Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection},
    author={Kai Greshake and Sahar Abdelnabi and Shailesh Mishra and Christoph Endres and Thorsten Holz and Mario Fritz},
    year={2023},
    eprint={2302.12173},
    archivePrefix={arXiv},
    primaryClass={cs.CR}
}

% injection definitions
@article{huang2023survey,
    title={A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation},
    author={Xiaowei Huang and Wenjie Ruan and Wei Huang and Gaojie Jin and Yi Dong and Changshun Wu and Saddek Bensalem and Ronghui Mu and Yi Qi and Xingyu Zhao and Kaiwen Cai and Yanghao Zhang and Sihao Wu and Peipei Xu and Dengyu Wu and Andre Freitas and Mustafa A. Mustafa},
    year={2023},
    eprint={2305.11391},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article{dinan2019build,
    title={Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack},
    author={Emily Dinan and Samuel Humeau and Bharath Chintagunta and Jason Weston},
    year={2019},
    eprint={1908.06083},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

% leaking, goal hijacking
@article{perez2022ignore,
    title={Ignore Previous Prompt: Attack Techniques For Language Models},
    author={Fábio Perez and Ian Ribeiro},
    year={2022},
    eprint={2211.09527},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

% obfuscation, etc.
@article{kang2023exploiting,
  title={Exploiting programmatic behavior of llms: Dual-use through standard security attacks},
  author={Kang, Daniel and Li, Xuechen and Stoica, Ion and Guestrin, Carlos and Zaharia, Matei and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2302.05733},
  year={2023}
}

% fundamental limitations on mitigating
@article{wolf2023fundamental,
  title={Fundamental limitations of alignment in large language models},
  author={Wolf, Yotam and Wies, Noam and Levine, Yoav and Shashua, Amnon},
  journal={arXiv preprint arXiv:2304.11082},
  year={2023}
}

% multistep jailbreaking
@article{li2023multistep,
    title={Multi-step Jailbreaking Privacy Attacks on ChatGPT},
    author={Haoran Li and Dadi Guo and Wei Fan and Mingshi Xu and Jie Huang and Fanpu Meng and Yangqiu Song},
    year={2023},
    eprint={2304.05197},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

% definitions, taxonomy, very good related work
% says jailbreak=injection
@article{rao2023tricking,
    title={Tricking LLMs into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks},
    author={Abhinav Rao and Sachin Vashistha and Atharva Naik and Somak Aditya and Monojit Choudhury},
    year={2023},
    eprint={2305.14965},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

% datasheets
@article{gebru2021datasheets,
   title={Datasheets for datasets},
   volume={64},
   ISSN={1557-7317},
   url={http://dx.doi.org/10.1145/3458723},
   DOI={10.1145/3458723},
   number={12},
   journal={Communications of the ACM},
   publisher={Association for Computing Machinery (ACM)},
   author={Gebru, T. and Morgenstern, J. and Vecchione, B. and Vaughan, J. Wortman and Wallach, H. and Daumé III, H. and Crawford, K.},
   year={2021},
   pages={86–92} }

% houyi injection
% define prompt abuse
@article{liu2023prompt,
    title={Prompt Injection attack against LLM-integrated Applications},
    author={Yi Liu and Gelei Deng and Yuekang Li and Kailong Wang and Tianwei Zhang and Yepang Liu and Haoyu Wang and Yan Zheng and Yang Liu},
    year={2023},
    eprint={2306.05499},
    archivePrefix={arXiv},
    primaryClass={cs.CR}
}


@software{Schulhoff_Learn_Prompting_2022,
author = {Schulhoff, Sander},
month = dec,
title = {{Learn Prompting}},
url = {https://github.com/trigaten/Learn_Prompting},
year = {2022}
}

@article{bai2022training,
    title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
    author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
    year={2022},
    eprint={2204.05862},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

% llms
@article{ouyang2022training,
    title={Training language models to follow instructions with human feedback},
    author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
    year={2022},
    eprint={2203.02155},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{chung2022scaling,
    title={Scaling Instruction-Finetuned Language Models},
    author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
    year={2022},
    eprint={2210.11416},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{openai2023gpt4,
    title={GPT-4 Technical Report},
    author={OpenAI},
    year={2023},
    eprint={2303.08774},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{guan2023cohortgpt,
  title={Cohortgpt: An enhanced gpt for participant recruitment in clinical study},
  author={Guan, Zihan and Wu, Zihao and Liu, Zhengliang and Wu, Dufan and Ren, Hui and Li, Quanzheng and Li, Xiang and Liu, Ninghao},
  journal={arXiv preprint arXiv:2307.11346},
  year={2023}
}

@article{liu2023summary,
  title={Summary of ChatGPT/GPT-4 research and perspective towards the future of large language models},
  author={Liu, Yiheng and Han, Tianle and Ma, Siyuan and Zhang, Jiayue and Yang, Yuanyuan and Tian, Jiaming and He, Hao and Li, Antong and He, Mengshen and Liu, Zhengliang and others},
  journal={arXiv preprint arXiv:2304.01852},
  year={2023}
}

@article{Liu2021PretrainPA,
  title={Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  author={Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
  journal={ACM Computing Surveys},
  year={2021},
  volume={55},
  pages={1 - 35}
}


@article{Brown2020LanguageMA,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and T. J. Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165}
}



@inproceedings{Liu2021WhatMG,
  title={What Makes Good In-Context Examples for GPT-3?},
  author={Jiachang Liu and Dinghan Shen and Yizhe Zhang and Bill Dolan and Lawrence Carin and Weizhu Chen},
  booktitle={Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out},
  year={2021}
}

@article{Lu2021FantasticallyOP,
  title={Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity},
  author={Yao Lu and Max Bartolo and Alastair Moore and Sebastian Riedel and Pontus Stenetorp},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.08786}
}



@article{Wei2022ChainOT,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed Huai-hsin Chi and F. Xia and Quoc Le and Denny Zhou},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.11903}
}

@article{qi2023visual,
  title={Visual Adversarial Examples Jailbreak Large Language Models},
  author={Qi, Xiangyu and Huang, Kaixuan and Panda, Ashwinee and Wang, Mengdi and Mittal, Prateek},
  journal={arXiv preprint arXiv:2306.13213},
  year={2023}
}

@article{fumisusing,
  title={MISUSING TOOLS IN LARGE LANGUAGE MODELS WITH VISUAL ADVERSARIAL EXAMPLES},
  author={Fu, Xiaohan and Wang, Zihan and Li, Shuheng and Gupta, Rajesh K and Mireshghallah, Niloofar and Berg-Kirkpatrick, Taylor and Fernandes, Earlence}
}

@inproceedings{schlarmann2023adversarial,
  title={On the adversarial robustness of multi-modal foundation models},
  author={Schlarmann, Christian and Hein, Matthias},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3677--3685},
  year={2023}
}

@article{carlini2023aligned,
  title={Are aligned neural networks adversarially aligned?},
  author={Carlini, Nicholas and Nasr, Milad and Choquette-Choo, Christopher A and Jagielski, Matthew and Gao, Irena and Awadalla, Anas and Koh, Pang Wei and Ippolito, Daphne and Lee, Katherine and Tramer, Florian and others},
  journal={arXiv preprint arXiv:2306.15447},
  year={2023}
}

@article{shayegani2023plug,
  title={Plug and Pray: Exploiting off-the-shelf components of Multi-Modal Models},
  author={Shayegani, Erfan and Dong, Yue and Abu-Ghazaleh, Nael},
  journal={arXiv preprint arXiv:2307.14539},
  year={2023}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{zhou2023advclip,
  title={Advclip: Downstream-agnostic adversarial examples in multimodal contrastive learning},
  author={Zhou, Ziqi and Hu, Shengshan and Li, Minghui and Zhang, Hangtao and Zhang, Yechao and Jin, Hai},
  journal={arXiv preprint arXiv:2308.07026},
  year={2023}
}

@article{greshake2023more,
  title={More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models},
  author={Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  journal={arXiv preprint arXiv:2302.12173},
  year={2023}
}

@article{bagdasaryan2023ceci,
  title={Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings},
  author={Bagdasaryan, Eugene and Shmatikov, Vitaly},
  journal={arXiv preprint arXiv:2308.11804},
  year={2023}
}

@article{bagdasaryan2023ab,
  title={(Ab) using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs},
  author={Bagdasaryan, Eugene and Hsieh, Tsung-Yin and Nassi, Ben and Shmatikov, Vitaly},
  journal={arXiv preprint arXiv:2307.10490},
  year={2023}
}


@article{Kojima2022LargeLM,
  title={Large Language Models are Zero-Shot Reasoners},
  author={Takeshi Kojima and Shixiang Shane Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.11916}
}

@article{nin2023new,
    title={New Jailbreak Based on Virtual Functions - Smuggle Illegal Tokens to the Backend.},
    author={u/Nin\_kat},
    url={https://www.reddit.com/r/ChatGPT/comments/10urbdj/new_jailbreak_based_on_virtual_functions_smuggle},
    year={2023},
}

@article{Zhou2022LeasttoMostPE,
  title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  author={Denny Zhou and Nathanael Scharli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and Dale Schuurmans and Olivier Bousquet and Quoc Le and Ed Huai-hsin Chi},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.10625}
}


@article{Khot2022DecomposedPA,
  title={Decomposed Prompting: A Modular Approach for Solving Complex Tasks},
  author={Tushar Khot and H. Trivedi and Matthew Finlayson and Yao Fu and Kyle Richardson and Peter Clark and Ashish Sabharwal},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.02406}
}

@article{Madaan2023SelfRefineIR,
  title={Self-Refine: Iterative Refinement with Self-Feedback},
  author={Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Sean Welleck and Bodhisattwa Prasad Majumder and Shashank Gupta and Amir Yazdanbakhsh and Peter Clark},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.17651}
}

@inproceedings{Shinn2023ReflexionLA,
  title={Reflexion: Language Agents with Verbal Reinforcement Learning},
  author={Noah Shinn and Federico Cassano and Beck Labash and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
  booktitle={NeurIPS},
  year={2023}
}


@inproceedings{Si2022PromptingGT,
  title={Prompting GPT-3 To Be Reliable},
  author={Chenglei Si and Zhe Gan and Zhengyuan Yang and Shuohang Wang and Jianfeng Wang and Jordan L. Boyd-Graber and Lijuan Wang},
  booktitle={ICLR},
  year={2023},
  url={https://arxiv.org/pdf/2210.09150.pdf}
}


% palantir
@article{palantir,
    url={https://www.palantir.com/platforms/aip/},
    title={Palantir AIP},
    year={2023}
}

% scale
@article{
scale,
url={https://scale.com/donovan},
title={Donovan},
year={2023}
}


@article{Wang2023OnTR,
  title={On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective},
  author={Jindong Wang and Xixu Hu and Wenxin Hou and Hao Chen and Runkai Zheng and Yidong Wang and Linyi Yang and Haojun Huang and Weirong Ye and Xiubo Geng and Binxing Jiao and Yue Zhang and Xingxu Xie},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.12095}
}


@article{Zhu2023PromptBenchTE,
  title={PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts},
  author={Kaijie Zhu and Jindong Wang and Jiaheng Zhou and Zichen Wang and Hao Chen and Yidong Wang and Linyi Yang and Weirong Ye and Neil Zhenqiang Gong and Yue Zhang and Xingxu Xie},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.04528}
}

@article{Ganguli2022RedTL,
  title={Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned},
  author={Deep Ganguli and Liane Lovitt and John Kernion and Amanda Askell and Yuntao Bai and Saurav Kadavath and Benjamin Mann and Ethan Perez and Nicholas Schiefer and Kamal Ndousse and Andy Jones and Sam Bowman and Anna Chen and Tom Conerly and Nova DasSarma and Dawn Drain and Nelson Elhage and Sheer El-Showk and Stanislav Fort and Zachary Dodds and T. J. Henighan and Danny Hernandez and Tristan Hume and Josh Jacobson and Scott Johnston and Shauna Kravec and Catherine Olsson and Sam Ringer and Eli Tran-Johnson and Dario Amodei and Tom B. Brown and Nicholas Joseph and Sam McCandlish and Christopher Olah and Jared Kaplan and Jack Clark},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.07858}
}



@inproceedings{Perez2022RedTL,
  title={Red Teaming Language Models with Language Models},
  author={Ethan Perez and Saffron Huang and Francis Song and Trevor Cai and Roman Ring and John Aslanides and Amelia Glaese and Nathan McAleese and Geoffrey Irving},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2022}
}



@article{AIindex,
  title={The AI Index 2023 Annual Report},
  author={Nestor Maslej and Loredana Fattorini and Erik Brynjolfsson and John Etchemendy and Katrina Ligett and Terah Lyons and
James Manyika and Helen Ngo and Juan Carlos Niebles and Vanessa Parli and Yoav Shoham and Russell Wald and Jack Clark and Raymond Perrault},
  year={2023},
}


@inproceedings{Carlini2020ExtractingTD,
  title={Extracting Training Data from Large Language Models},
  author={Nicholas Carlini and Florian Tram{\`e}r and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom B. Brown and Dawn Xiaodong Song and {\'U}lfar Erlingsson and Alina Oprea and Colin Raffel},
  booktitle={USENIX Security Symposium},
  year={2020}
}

@article{Shaikh2022OnST,
  title={On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning},
  author={Shaikh, Omar and Zhang, Hongxin and Held, William and Bernstein, Michael and Yang, Diyi},
  journal={arXiv preprint arXiv:2212.08061},
  year={2022}
}


@inproceedings{gehman-etal-2020-realtoxicityprompts,
    title = "{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models",
    author = "Gehman, Samuel  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.301",
    doi = "10.18653/v1/2020.findings-emnlp.301",
    pages = "3356--3369",
    abstract = "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning {``}bad{''} words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
}


@article{Liu2023JailbreakingCV,
  title={Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study},
  author={Yi Liu and Gelei Deng and Zhengzi Xu and Yuekang Li and Yaowen Zheng and Ying Zhang and Lida Zhao and Tianwei Zhang and Yang Liu},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.13860}
}


@article{Gao2022PALPL,
  title={PAL: Program-aided Language Models},
  author={Luyu Gao and Aman Madaan and Shuyan Zhou and Uri Alon and Pengfei Liu and Yiming Yang and Jamie Callan and Graham Neubig},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.10435}
}

@article{Ribeiro2020BeyondAB,
  title={Beyond Accuracy: Behavioral Testing of NLP Models with CheckList},
  author={Marco Tulio Ribeiro and Tongshuang Sherry Wu and Carlos Guestrin and Sameer Singh},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.04118}
}


@article{Vilar2022PromptingPF,
  title={Prompting PaLM for Translation: Assessing Strategies and Performance},
  author={David Vilar and Markus Freitag and Colin Cherry and Jiaming Luo and Viresh Ratnakar and George F. Foster},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.09102}
}


@article{Scao2022BLOOMA1,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}


@article{Bommasani2021OnTO,
  title={On the Opportunities and Risks of Foundation Models},
  author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and S. Buch and Dallas Card and Rodrigo Castellon and Niladri S. Chatterji and Annie S. Chen and Kathleen A. Creel and Jared Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren E. Gillespie and Karan Goel and Noah D. Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas F. Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and O. Khattab and Pang Wei Koh and Mark S. Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Benjamin Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and J. F. Nyarko and Giray Ogut and Laurel J. Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Robert Reich and Hongyu Ren and Frieda Rong and Yusuf H. Roohani and Camilo Ruiz and Jack Ryan and Christopher R'e and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishna Parasuram Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tram{\`e}r and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei A. Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.07258}
}

% restricting length
@article{selvi2022exploring,
    author={Jose Selvi},
    title={Exploring Prompt Injection Attacks},
    year={2022},
    month={Dec},
    day={5},
    url={https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/}
}

@article{microsoft2023bing,
author={Microsoft},
title={The New Bing and Edge - Updates to Chat},
month={2},
day={7},
year={2023},
url={https://blogs.bing.com/search/february-2023/The-new-Bing-Edge-\%E2\%80\%93-Updates-to-Chat}
}

% term template
@article{sorensen2022an,
   title={An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels},
   url={http://dx.doi.org/10.18653/v1/2022.acl-long.60},
   DOI={10.18653/v1/2022.acl-long.60},
   journal={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
   publisher={Association for Computational Linguistics},
   author={Sorensen, Taylor and Robinson, Joshua and Rytting, Christopher and Shaw, Alexander and Rogers, Kyle and Delorey, Alexia and Khalil, Mahmoud and Fulda, Nancy and Wingate, David},
   year={2022} }

% reconstruction
@article{rigaki2020survey,
    title={A Survey of Privacy Attacks in Machine Learning},
    author={Maria Rigaki and Sebastian Garcia},
    year={2020},
    eprint={2007.07646},
    archivePrefix={arXiv},
    primaryClass={cs.CR}
}

% prompting is hard
@inproceedings{pereira2023why,
author = {Zamfirescu-Pereira, J.D. and Wong, Richmond Y. and Hartmann, Bjoern and Yang, Qian},
title = {Why Johnny Can’t Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581388},
doi = {10.1145/3544548.3581388},
abstract = {Pre-trained large language models (“LLMs”) like GPT-3 can engage in fluent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer LLM outputs (“prompting”) has emerged as an important design technique potentially accessible to non-AI-experts. Crafting effective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-AI-experts can successfully engage in “end-user prompt engineering” using a design probe—a prototype LLM-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to effective prompt design. These findings have implications for non-AI-expert-facing LLM-based tool design and for improving LLM-and-prompt literacy among programmers and the public, and present opportunities for further research.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {437},
numpages = {21},
keywords = {end-users, design tools, language models},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{khashabi2022prompt,
   title={Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts},
   url={http://dx.doi.org/10.18653/v1/2022.naacl-main.266},
   DOI={10.18653/v1/2022.naacl-main.266},
   journal={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
   publisher={Association for Computational Linguistics},
   author={Khashabi, Daniel and Lyu, Xinxi and Min, Sewon and Qin, Lianhui and Richardson, Kyle and Welleck, Sean and Hajishirzi, Hannaneh and Khot, Tushar and Sabharwal, Ashish and Singh, Sameer and Choi, Yejin},
   year={2022} }

@article{min2022rethinking,
    title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
    author={Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
    year={2022},
    eprint={2202.12837},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{webson2021prompt,
  title={Do prompt-based models really understand the meaning of their prompts?},
  author={Webson, Albert and Pavlick, Ellie},
  journal={arXiv preprint arXiv:2109.01247},
  year={2021}
}

@article{hu2023promptbased,
    title={Prompt-based methods may underestimate large language models' linguistic generalizations},
    author={Jennifer Hu and Roger Levy},
    year={2023},
    eprint={2305.13264},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{prasad-etal-2023-grips,
    title = "{G}r{IPS}: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models",
    author = "Prasad, Archiki  and
      Hase, Peter  and
      Zhou, Xiang  and
      Bansal, Mohit",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.277",
    pages = "3845--3864",
    abstract = "Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural Instructions dataset (with similar improvements for OPT, BLOOM, and FLAN-T5). We see improvements for both instruction-only prompts and instruction + k-shot examples prompts. Notably, GrIPS outperforms manual rewriting and purely example-based prompts while controlling for the available compute and data budget. Further, performance of GrIPS is comparable to select gradient-based tuning approaches. Qualitatively, we show our edits can simplify instructions and at times make them incoherent but nonetheless improve accuracy.",
}

% define prompting
@inproceedings{shin-etal-2020-autoprompt,
    title = "{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts",
    author = "Shin, Taylor  and
      Razeghi, Yasaman  and
      Logan IV, Robert L.  and
      Wallace, Eric  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.346",
    doi = "10.18653/v1/2020.emnlp-main.346",
    pages = "4222--4235",
    abstract = "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.",
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}



@book{yu2013distributed,
author = {Yu, Shui},
title = {Distributed Denial of Service Attack and Defense},
year = {2013},
isbn = {1461494907},
publisher = {Springer Publishing Company, Incorporated},
abstract = {This brief provides readers a complete and self-contained resource for information about DDoS attacks and how to defend against them. It presents the latest developments in this increasingly crucial field along with background context and survey material. The book also supplies an overview of DDoS attack issues, DDoS attack detection methods, DDoS attack source traceback, and details on how hackers organize DDoS attacks. The author concludes with future directions of the field, including the impact of DDoS attacks on cloud computing and cloud technology. The concise yet comprehensive nature of this brief makes it an ideal reference for researchers and professionals studying DDoS attacks. It is also a useful resource for graduate students interested in cyberterrorism and networking.}
}

% 
@article{
    spacekangaroo,
    author = {Christopher R. Carnahan},
    title = {How a \$5000 Prompt Injection Contest Helped Me Become a Better Prompt Engineer},
    year = {2023},
    url = {https://www.spacekangaroo.ai/post/how-a-5000-prompt-injection-contest-helped-me-become-a-better-prompt-engineer}
}

@article{wyk2023protect,
    title={Protect Your Prompts: Protocols for IP Protection in LLM Applications},
    author={M. A. van Wyk and M. Bekker and X. L. Richards and K. J. Nixon},
    year={2023},
    eprint={2306.06297},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{jessica2023solid,
    title={SolidGoldMagikarp (plus, prompt generation)},
    author={Jessica Rumbelow and mwatkins},
    day={5},
    month={feb},
    year={2023},
    url={https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation},
}

@article{yedidia2023anomalous,
    title={SmartyHeaderCode: anomalous tokens for GPT3.5 and GPT-4},
    author={Adam Yedidia},
    url={https://www.lesswrong.com/posts/ChtGdxk9mwZ2Rxogt/smartyheadercode-anomalous-tokens-for-gpt3-5-and-gpt-4-1},
    year={2023},
    day={15},
    month={April}
}

% few shot
@article{logan2021cutting,
      title={Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models}, 
      author={Robert L. Logan and Ivana Balažević and Eric Wallace and Fabio Petroni and Sameer Singh and Sebastian Riedel},
      year={2021},
      eprint={2106.13353},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{lake2015human,
  title={Human-level concept learning through probabilistic program induction},
  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  journal={Science},
  volume={350},
  number={6266},
  pages={1332--1338},
  year={2015},
  publisher={American Association for the Advancement of Science}
}

@article{willison2023the,
    title={The Dual LLM pattern for building AI assistants that can resist prompt injection},
    url={https://simonwillison.net/2023/Apr/25/dual-llm-pattern/},
    author={Simon Willison},
    day={25},
    month={april},
    year={2023}
}

@article{ludwig2023achieving,
    title={Achieving Code Execution in MathGPT via Prompt Injection},
    url={https://atlas.mitre.org/studies/AML.CS0016/},
    month={Jan},
    day={28},
    year={2023},
    author={Stumpp, Ludwig-Ferdinand}
}

@software{dinu2023NeMo,
    title={NeMo-Guardrails},
    year={2023},
    author={Razvan Dinu and hongyishi},
    url={https://github.com/NVIDIA/NeMo-Guardrails}
}

@article{kirichenko2020best,
  title={Best practices and recommendations for cybersecurity service providers},
  author={Kirichenko, Alexey and Christen, Markus and Grunow, Florian and Herrmann, Dominik},
  journal={The ethics of cybersecurity},
  pages={299--316},
  year={2020},
  publisher={Springer International Publishing}
}

@article{cencini2005software,
  title={Software vulnerabilities: full-, responsible-, and non-disclosure},
  author={Cencini, Andrew and Yu, Kevin and Chan, Tony},
  journal={December},
  volume={7},
  pages={10},
  year={2005}
}

@article{alex2023jailbroken,
    title={Jailbroken: How Does LLM Safety Training Fail?},
    author={Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
    year={2023},
    eprint={2307.02483},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{yong2023lowresource,
    title={Low-Resource Languages Jailbreak GPT-4},
    author={Zheng-Xin Yong and Cristina Menghini and Stephen H. Bach},
    year={2023},
    eprint={2310.02446},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{wei2023jailbreak,
    title={Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations},
    author={Zeming Wei and Yifei Wang and Yisen Wang},
    year={2023},
    eprint={2310.06387},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@article{Shen2023DoAN,
  title={"Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models},
  author={Xinyu Shen and Zeyuan Johnson Chen and Michael Backes and Yun Shen and Yang Zhang},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.03825},
  url={https://api.semanticscholar.org/CorpusID:260704242}
}


@article{chen2023chatgpts,
    title={How is ChatGPT's behavior changing over time?},
    author={Lingjiao Chen and Matei Zaharia and James Zou},
    year={2023},
    eprint={2307.09009},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{shen2023characterizing,
    title={"Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models},
    author={Xinyue Shen and Zeyuan Chen and Michael Backes and Yun Shen and Yang Zhang},
    year={2023},
    eprint={2308.03825},
    archivePrefix={arXiv},
    primaryClass={cs.CR}
}

 @website{Gandalf2023, 
title={Your goal is to make Gandalf reveal the secret password for each level},
author={Lakera},
year={2023},
url={https://gandalf.lakera.ai/}, abstractNote={Trick Gandalf into revealing information and experience the limitations of large language models firsthand.}, language={en} }

@article{fangzhao2023defending,
  title={Defending ChatGPT against Jailbreak Attack via Self-Reminder},
  author={Yueqi Xie and Jingwei Yi and Jiawei Shao and Justin Curl and Lingjuan Lyu and Qifeng Chen and Xing Xie and Fangzhao Wu},
  journal={Physical Sciences - Article},
  year={2023},
  month={June},
  day={16},
  publisher={Microsoft Research Asia},
  doi={10.21203/rs.3.rs-2873090/v1},
  license={Creative Commons Attribution 4.0 International License},
  note={Joint First Authors},
  url={https://doi.org/10.21203/rs.3.rs-2873090/v1}
}


@article{karpas2022mrkl,
    title={MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning},
    author={Ehud Karpas and Omri Abend and Yonatan Belinkov and Barak Lenz and Opher Lieber and Nir Ratner and Yoav Shoham and Hofit Bata and Yoav Levine and Kevin Leyton-Brown and Dor Muhlgay and Noam Rozen and Erez Schwartz and Gal Shachaf and Shai Shalev-Shwartz and Amnon Shashua and Moshe Tenenholtz},
    year={2022},
    eprint={2205.00445},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@software{hack-a-prompt,
  author = {Terjanq},
    url={https://github.com/terjanq/hack-a-prompt},
  title = {HackAPrompt 2023},
  year = {2023},
  note = {GitHub repository},
}
