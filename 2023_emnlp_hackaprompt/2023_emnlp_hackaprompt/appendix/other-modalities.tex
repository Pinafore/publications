\section{Injections in Other Modalities}
\label{appx:other-modalities}

Prompt hacking does not stop with text. It can be generalized to other modalities and hurt end users in different ways~\cite{schlarmann2023adversarial}. Generative models ingesting or producing sound, images, and video outputs are at risk.

Injections can be placed directly into images or sound clips. Attackers have already blended malicious prompts into images or sounds provided to the model, steering it to output the attacker-chosen text \cite{bagdasaryan2023ab,fumisusing,qi2023visual,carlini2023aligned}.

Related work on adversarial illusions~\cite{zhou2023advclip,shayegani2023plug,bagdasaryan2023ceci} may also be relevant. In this process, an attacker perturbs the embedding space by sending a modified \textit{adversarial} input.


