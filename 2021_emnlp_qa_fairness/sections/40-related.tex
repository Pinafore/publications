
\section{Related Work}
\label{sec:related}

Language is a reflection of culture.
Like other cultural artifacts---encyclopedias~\cite{reagle-11}, and films~\cite{sap-etal-2017-connotation}---\abr{qa} has more men than women.
Other artifacts like children's books have more gender balance but reflect other aspects of culture~\cite{larrick-65}.

The \abr{nlp} literature is also grappling with demographic discrepancies.
Standard coreference systems falter on gender-balanced corpora~\cite{webster-18}, and \citet{zhao-18} create synthetic training data to reduce bias.
Similar coreference issues plague machine translation systems~\cite{stanovsky-19}, and \citet{li-20} use \abr{qa} to probe biases of \abr{nlp} systems.
\citet{Sen_2020} show that there are shortcomings in \qa{} datasets and evaluations by analysing their out-of-domain generalization capabilities and ability to handle question variation.
Joint models of vision and language suggest that biases come from language, rather than from  vision~\cite{ross-etal-2021-measuring}.
However, despite a range of mitigation techniques~\citep[inter alia]{zhao-17} none, to our knowledge, have been successfully applied to \abr{qa}, especially from the demographic viewpoint.

